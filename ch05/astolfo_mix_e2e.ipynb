{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AstolfoMix E2E merger #\n",
    "\n",
    "Extensive use of \"N-Average\" and \"Split UNET / TE\".\n",
    "\n",
    "## Abstract ##\n",
    "\n",
    "- Base code: [n_average.py](https://github.com/ljleb/sd-mecha/blob/main/examples/n_average.py), [split_unet_text_encoder.py](https://github.com/ljleb/sd-mecha/blob/main/examples/split_unet_text_encoder.py) and [serialize_recipe.py](https://github.com/ljleb/sd-mecha/blob/main/examples/serialize_recipe.py)\n",
    "- Using [sd-mecha](https://github.com/ljleb/sd-mecha) as main library. **Thank you [@ljleb](https://github.com/ljleb/) for the codebase, and [@illyaeater](https://github.com/Enferlain) for the alpha tester.**\n",
    "- Each generated model will have its own model metadata and `*.mecha` ~~assembly like~~ [recipe](https://github.com/ljleb/sd-mecha/blob/main/examples/recipes/test_split_unet_text_encoder.mecha). Open it with text editor.\n",
    "- **No need to waste 1TB+ of disk space for pariwise merging and iterlate with WebUI.** However you should know the \"model pool\", otherwise it is likely result in a worse model. \n",
    "- Required time: 25 (RAW) + 150 * 4 (TE) + 25 (SELECTED_TE) + 50 * 4 (UNET) + 25 (FINAL) + 45 (E2E) minutes = **almost 16 hours** for 52 models\n",
    "- CPU usage: **100% with AVX2.**\n",
    "- RAM usage: *Around 32GB*.\n",
    "- VRAM usage: *Around 4GB*. \n",
    "- Storage usage: $5N+3$ SDXL models, including $N$ raw models. For $N=52$, it will use **1.66TB** for the most efficient approach.\n",
    "- I intentionally make it into Python notebook because I need to switch mode this time.\n",
    "\n",
    "## Required libraries ##\n",
    "\n",
    "- `torch>=2.0.1`\n",
    "- `tensordict`\n",
    "- `sd-mecha` (I prefer [clone](https://github.com/ljleb/sd-mecha/tree/main) the source code inplace,current version as on 240222, commit `afdab8b003730f58b9127228ef68b0014a3c487d`)\n",
    "- [safetensors](https://huggingface.co/docs/safetensors/index)\n",
    "- [diffusers](https://huggingface.co/docs/diffusers/installation)\n",
    "- [pytorch](https://pytorch.org/get-started/locally/#windows-python)\n",
    "\n",
    "## Model naming schema ##\n",
    "\n",
    "- `RAW` as `_x01`: Place all raw models. Will generate `x51a` as averaged model regardless components.\n",
    "- `CLIP` as `_x01te`: Will generate all models as `x51a` replaced with `_x01`'s TE. Will be a set of `te0`, `te1`, `te2`. Use these models for model selection. \n",
    "- `UNET` as `x51a-x39te0x39te1`: *Require selected TEs.* Will generate all models as `_01`'s UNET and average of selected `te0` and `te1`. VAE will be `x51a`.\n",
    "- `FINAL` as `e2e`: Final model as `x45`.\n",
    "\n",
    "## Recommened directories to make ##\n",
    "\n",
    "- `raw`: Store the raw $N$ models\n",
    "- `clip`: Store $3N$ models for CLIP selection\n",
    "- `unet`: Store $N$ models for UNET selection\n",
    "\n",
    "## Operation Mode ##\n",
    "\n",
    "- [`RAW`, `CLIP`, `UNET`, `FINAL`]. Procedure will be *mutually exclusive*. I will keep restarting the whole notebook.\n",
    "\n",
    "## Limitation ##\n",
    "\n",
    "- ~~VAE remains unmanaged.~~ VAE can be picked from one of the raw models.\n",
    "- SDXL models only. I don't need this for SD1 and SD2.\n",
    "- Safetensors only. \n",
    "\n",
    "## WTF why and will it work? ##\n",
    "\n",
    "- Yes. [It is part of my research](./README_XL.md).\n",
    "- Image comparasion will be listed there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Pseudocode of sd-mecha ##\n",
    "\n",
    "- Note that the core concept is different from WebUI or supermerger. It focus on [serialization](https://www.geeksforgeeks.org/serialization-in-java/), along with *multiple merging methods* and *custom applied areas*.\n",
    "\n",
    "- It will pick `model_b_as_recipe` for every merge key and `model_a` for every passthroguh key\n",
    "\n",
    "- Sample code: `model_b_as_recipe = sd_mecha.merge_methods(model_a, model_b_as_recipe, alpha, beta, etc) #returns model_a`\n",
    "\n",
    "- For example, in `n_average`, `alpha` tends to `1`, instead of `0` in WebUI. \n",
    "\n",
    "- Also, `pick_vae` will show a special case on \"bake VAE\": `model_a_instead = sd_mecha.merge_methods(model_a, model_b_as_recipe, alpha=1) #returns model_a`\n",
    "\n",
    "Algorithm `SD-MECHA`:\n",
    "\n",
    "------\n",
    "\n",
    "- Let\n",
    "\n",
    "$\\{model_A, model_B\\, model_C\\} \\in models$ and $arch_{models} \\in arch_{SD}$ and is consistant (i.e. $arch_{model_A}=arch_{model_B}$)\n",
    "\n",
    "$\\{SumAverage,AddDiff,Rotate,ReBasin, etc.\\} \\in merge$\n",
    "\n",
    "$\\{CLIP, UNET, VAE\\} \\in models$, but $\\{CLIP, UNET\\} \\in \\alpha, \\{VAE\\} \\notin \\alpha$\n",
    "\n",
    "$ \\alpha = [0,1] , \\alpha=0 \\implies model_A, \\alpha=1 \\implies model_B$\n",
    "\n",
    "- Repeat:\n",
    "\n",
    "$model_A, model_B, merge, \\alpha, \\beta, etc. \\leftarrow deserialize(recipe)$ or user defined\n",
    "\n",
    "$model_A \\leftarrow merge(model_A, model_B, \\alpha, \\beta, etc.)$\n",
    "\n",
    "$model_B \\leftarrow model_A$\n",
    "\n",
    "$recipe \\leftarrow serialize(model_B)$\n",
    "\n",
    "- Until no more $model_B$\n",
    "\n",
    "- Return $recipe$\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Is dependency fufilled?\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the main module.\n",
    "import sd_mecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for OMP: Error #15\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User input session starts here. ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify all the paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_BASE = \"../../stable-diffusion-webui/tmp/astolfo_mix/sdxl/\" #To set up merger\n",
    "\n",
    "DIR_RAW = \"raw/\" #To load N models\n",
    "DIR_CLIP = \"clip/\"  #To write 3N models\n",
    "DIR_UNET = \"unet/\" #To write N models\n",
    "DIR_FINAL = \"./\" #To write 1 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check on directory and make the model name prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MECHA_RECIPE_EXT = \".mecha\"\n",
    "MECHA_MODEL_EXT = \".safetensors\"\n",
    "\n",
    "MODEL_LIST_RAW = os.listdir(\"{}{}\".format(DIR_BASE,DIR_RAW))\n",
    "# Exclude yaml.\n",
    "MODEL_LIST_RAW = list(filter(lambda p: p.endswith(MECHA_MODEL_EXT), MODEL_LIST_RAW)) #p.endswith(\".ckpt\") or p.endswith(\".safetensors\") or p.endswith(\".bin\")\n",
    "if len(MODEL_LIST_RAW) < 2:\n",
    "    raise Exception(\"Need at least 2 models for merge.\")\n",
    "#model_list = list(map(lambda p: os.path.splitext(os.path.basename(p))[0], model_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 raw models found.\n"
     ]
    }
   ],
   "source": [
    "print(\"{} raw models found.\".format(len(MODEL_LIST_RAW)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model selection. Index start with 1. Check model list for ordering!\n",
    "```\n",
    "te0: --,--,07,--,--,--,--,15,--,--,--,--,29,--,--,--,--,40,--,44,48,--=-6\n",
    "te1: --,--,--,09,--,--,--,15,--,--,--,--,29,--,31,34,--,--,--,--,48,--=-6\n",
    "te2: --,06,--,09,--,11,12,15,--,--,--,--,29,30,31,34,38,40,42,44,48,49=-15\n",
    "=sd: 03,--,--,--,10,--,12,--,16,18,19,25=-7\n",
    "\n",
    "te0: 03,07,10,12,15,16,18,19,29,40,44,48=-12\n",
    "te1: 03,09,10,12,15,16,18,19,29,31,34,48=-12\n",
    "unet: 03,09,14,24,48,50=-6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SELECTION_TE0 = [i+1 for i in range(len(MODEL_LIST_RAW)) if i+1 not in [3,7,10,12,15,16,18,19,29,40,44,48]] \n",
    "MODEL_SELECTION_TE1 = [i+1 for i in range(len(MODEL_LIST_RAW)) if i+1 not in [3,9,10,12,15,16,18,19,29,31,34,48]] \n",
    "MODEL_SELECTION_UNET = [i+1 for i in range(len(MODEL_LIST_RAW)) if i+1 not in [3,9,14,24,48,50]]\n",
    "\n",
    "#25 is the Original SDXL\n",
    "MODEL_SELECTION_VAE = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TE0:40,TE1:40,UNET:46\n"
     ]
    }
   ],
   "source": [
    "print(\"TE0:{},TE1:{},UNET:{}\".format(len(MODEL_SELECTION_TE0),len(MODEL_SELECTION_TE1),len(MODEL_SELECTION_UNET)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify all the keywords (I'll avoid hardcode because they will be everywhere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE_RAW = 'MODE_RAW'\n",
    "MODE_CLIP = 'MODE_CLIP'\n",
    "MODE_UNET = 'MODE_UNET'\n",
    "MODE_FINAL = 'MODE_FINAL'\n",
    "\n",
    "MODE_ACTIVATED = [MODE_RAW,MODE_CLIP,MODE_UNET,MODE_FINAL] #[MODE_RAW,MODE_CLIP,MODE_UNET,MODE_FINAL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert version number, and the... *\"AstolfoMix\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME_SUFFIX = \"240222-60d0764\" #yymmdd-commit\n",
    "MODEL_NAME_KEYWORD = \"AstolfoMix\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change if your PC is in trouble.\n",
    "\n",
    "My PC: i9-7960X, X299 Dark, 128GB DDR4, 2x RTX3090, P4510. Overkill for a merger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_device = \"cuda:0\" #\"cpu\"\n",
    "g_precision_while_merge = torch.float64 if \"cuda\" in g_device else torch.float #I have RAM\n",
    "g_precision_final_model = torch.float16 if \"cuda\" in g_device else torch.float #fp16\n",
    "g_total_buffer_size=2**32 #4GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User input shuold ends here. ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define output model name. I want to keep the format, however I need to manage the name manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT_BYPASS = \"{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto zfill under total model count\n",
    "def az(n):\n",
    "    return str(n).zfill(math.ceil(math.log10(len(MODEL_LIST_RAW))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLIP = (\"te0\",\"te1\",\"te2\")\n",
    "N_RAW = \"_x{}\"\n",
    "N_ITR = \"{}a\"\n",
    "N_PICKED = \"x{}\"\n",
    "\n",
    "MODEL_NAME_RAW_PREFIX = (N_ITR.format(N_PICKED)).format(az(len(MODEL_LIST_RAW)-1)) #x49a\n",
    "\n",
    "MODEL_NAME_TE = (\"{}{}{}{}\".format(N_PICKED,N_CLIP[0],N_PICKED,N_CLIP[1])).format(az(len(MODEL_SELECTION_TE0)-1),az(len(MODEL_SELECTION_TE1)-1)) #x22te0x31te1\n",
    "MODEL_NAME_FINAL_PREFIX = N_PICKED.format(az(len(MODEL_SELECTION_UNET)-1)) #x43\n",
    "\n",
    "MODEL_NAME_RAW = \"{}-{}-{}\".format(MODEL_NAME_RAW_PREFIX,MODEL_NAME_KEYWORD,MODEL_NAME_SUFFIX) #x49a-AstolfoMix-e2e-240222-60d0764\n",
    "MODEL_NAME_TE_ITR = \"{}-{}-{}{}-{}\".format(MODEL_NAME_RAW_PREFIX,MODEL_NAME_KEYWORD,FORMAT_BYPASS,FORMAT_BYPASS,MODEL_NAME_SUFFIX) #x49a-AstolfoMix-_x01te0-e2e-240222-60d0764\n",
    "MODEL_NAME_SELECTED_TE = \"{}-{}-{}-{}\".format(MODEL_NAME_RAW_PREFIX,MODEL_NAME_KEYWORD,MODEL_NAME_TE,MODEL_NAME_SUFFIX) #x49a-AstolfoMix-x22te0x31te1-e2e-240222-60d0764\n",
    "MODEL_NAME_UNET_ITR = \"{}-{}-{}-{}\".format(FORMAT_BYPASS,MODEL_NAME_KEYWORD,MODEL_NAME_TE,MODEL_NAME_SUFFIX) #_x01a-AstolfoMix-x22te0x31te1-e2e-240222-60d0764\n",
    "MODEL_NAME_FINAL = \"{}-{}-{}-{}\".format(MODEL_NAME_FINAL_PREFIX,MODEL_NAME_KEYWORD,MODEL_NAME_TE,MODEL_NAME_SUFFIX) #x43-AstolfoMix-x22te0x31te1-e2e-240222-60d0764\n",
    "MODEL_NAME_E2E = \"{}-{}-{}-e2e-{}\".format(MODEL_NAME_FINAL_PREFIX,MODEL_NAME_KEYWORD,MODEL_NAME_TE,MODEL_NAME_SUFFIX) #x43-AstolfoMix-x22te0x31te1-e2e-240222-60d0764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive average model:                     x51a-AstolfoMix-240222-60d0764\n",
      "CLIP models to iterlate:                 x51a-AstolfoMix-{}{}-240222-60d0764\n",
      "Naive average model with selected CLIP:  x51a-AstolfoMix-x39te0x39te1-240222-60d0764\n",
      "UNET models to iterlate:                 {}-AstolfoMix-x39te0x39te1-240222-60d0764\n",
      "Final merged model (staged):             x45-AstolfoMix-x39te0x39te1-240222-60d0764\n",
      "Final merged model (e2e):                x45-AstolfoMix-x39te0x39te1-e2e-240222-60d0764\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive average model:                     {}\".format(MODEL_NAME_RAW))\n",
    "print(\"CLIP models to iterlate:                 {}\".format(MODEL_NAME_TE_ITR))\n",
    "print(\"Naive average model with selected CLIP:  {}\".format(MODEL_NAME_SELECTED_TE))\n",
    "print(\"UNET models to iterlate:                 {}\".format(MODEL_NAME_UNET_ITR))\n",
    "print(\"Final merged model (staged):             {}\".format(MODEL_NAME_FINAL))\n",
    "print(\"Final merged model (e2e):                {}\".format(MODEL_NAME_E2E))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up merge receipe and merge scheduler ##\n",
    "\n",
    "- I'm still a bit panic about hardcoding. Getter / Setter will be fine. ~~No, you won't see OOP in python notebook.~~\n",
    "- Will always run. `MODE_ACTIVATED` controls actual merge process only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmk_raw():\n",
    "    return 'RAW'\n",
    "def rmk_ste():\n",
    "    return 'SELECTED_TE'\n",
    "def rmk_f():\n",
    "    return 'FINAL'\n",
    "def rmk_e2e():\n",
    "    return 'E2E'\n",
    "def rmk_te(i,j):\n",
    "    return 'CLIP{}_TE{}'.format(i,j) #CLIP1TE0\n",
    "def rmk_unet(i):\n",
    "    return 'UNET{}'.format(i)  #UNET1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_mapping = {}\n",
    "\n",
    "def set_rmk(k, v):\n",
    "    recipe_mapping[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_rm():\n",
    "    set_rmk(rmk_raw(), None)\n",
    "    set_rmk(rmk_ste(), None)\n",
    "    set_rmk(rmk_f(), None)\n",
    "\n",
    "    for i in range(len(MODEL_LIST_RAW)):\n",
    "        set_rmk(rmk_unet(i+1), None)\n",
    "        for j in range(3):\n",
    "            set_rmk(rmk_te(i+1,j), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_rm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single merger should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = sd_mecha.RecipeMerger(\n",
    "    models_dir=DIR_BASE,\n",
    "    default_device=g_device,\n",
    "    default_dtype=g_precision_while_merge,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define recipe extension, and make the model output path (Note that it is still being formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "OS_MODEL_PATH_RAW = \"{}{}{}\".format(DIR_BASE,MODEL_NAME_RAW,MECHA_MODEL_EXT)\n",
    "RECIPE_PATH_RAW = \"{}{}{}\".format(DIR_BASE,MODEL_NAME_RAW,MECHA_RECIPE_EXT)\n",
    "MECHA_MODEL_PATH_RAW = \"{}{}\".format(DIR_FINAL,MODEL_NAME_RAW)\n",
    "\n",
    "OS_MODEL_PATH_TE_ITR = \"{}{}{}{}\".format(DIR_BASE,DIR_CLIP,MODEL_NAME_TE_ITR,MECHA_MODEL_EXT)\n",
    "RECIPE_PATH_TE_ITR = \"{}{}{}{}\".format(DIR_BASE,DIR_CLIP,MODEL_NAME_TE_ITR,MECHA_RECIPE_EXT)\n",
    "MECHA_MODEL_PATH_TE_ITR =  \"{}{}{}\".format(DIR_FINAL,DIR_CLIP,MODEL_NAME_TE_ITR)\n",
    "\n",
    "OS_MODEL_PATH_SELECTED_TE = \"{}{}{}\".format(DIR_BASE,MODEL_NAME_SELECTED_TE,MECHA_MODEL_EXT)\n",
    "RECIPE_PATH_SELECTED_TE = \"{}{}{}\".format(DIR_BASE,MODEL_NAME_SELECTED_TE,MECHA_RECIPE_EXT)\n",
    "MECHA_MODEL_PATH_SELECTED_TE =  \"{}{}\".format(DIR_FINAL,MODEL_NAME_SELECTED_TE)\n",
    "\n",
    "OS_MODEL_PATH_UNET_ITR = \"{}{}{}{}\".format(DIR_BASE,DIR_UNET,MODEL_NAME_UNET_ITR,MECHA_MODEL_EXT)\n",
    "RECIPE_PATH_UNET_ITR = \"{}{}{}{}\".format(DIR_BASE,DIR_UNET,MODEL_NAME_UNET_ITR,MECHA_RECIPE_EXT)\n",
    "MECHA_MODEL_PATH_UNET_ITR =  \"{}{}{}\".format(DIR_FINAL,DIR_UNET,MODEL_NAME_UNET_ITR)\n",
    "\n",
    "OS_MODEL_PATH_FINAL = \"{}{}{}\".format(DIR_BASE,MODEL_NAME_FINAL,MECHA_MODEL_EXT)\n",
    "RECIPE_PATH_FINAL = \"{}{}{}\".format(DIR_BASE,MODEL_NAME_FINAL,MECHA_RECIPE_EXT)\n",
    "MECHA_MODEL_PATH_FINAL = \"{}{}\".format(DIR_FINAL,MODEL_NAME_FINAL)\n",
    "\n",
    "OS_MODEL_PATH_E2E = \"{}{}{}\".format(DIR_BASE,MODEL_NAME_E2E,MECHA_MODEL_EXT)\n",
    "RECIPE_PATH_E2E = \"{}{}{}\".format(DIR_BASE,MODEL_NAME_E2E,MECHA_RECIPE_EXT)\n",
    "MECHA_MODEL_PATH_E2E = \"{}{}\".format(DIR_FINAL,MODEL_NAME_E2E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better test the ugly full file path\n",
    "def get_te_itr_path(s: str,i: int,j: int):\n",
    "    return s.format(N_RAW.format(az(i+1)),N_CLIP[j])\n",
    "\n",
    "def get_unet_itr_path(s: str,i: int):\n",
    "    return s.format((N_ITR.format(N_PICKED)).format(az(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample TE_ITR recipe path:   ../../stable-diffusion-webui/tmp/astolfo_mix/sdxl/clip/x51a-AstolfoMix-_x02te1-240222-60d0764.mecha\n",
      "Sample TE_ITR model path:    ../../stable-diffusion-webui/tmp/astolfo_mix/sdxl/clip/x51a-AstolfoMix-_x02te1-240222-60d0764.safetensors\n",
      "Sample UNET_ITR recipe path: ../../stable-diffusion-webui/tmp/astolfo_mix/sdxl/unet/x02a-AstolfoMix-x39te0x39te1-240222-60d0764.mecha\n",
      "Sample UNET_ITR model path:  ../../stable-diffusion-webui/tmp/astolfo_mix/sdxl/unet/x02a-AstolfoMix-x39te0x39te1-240222-60d0764.safetensors\n",
      "Does RAW model exists:       True\n",
      "Final model path:            ./x45-AstolfoMix-x39te0x39te1-240222-60d0764\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample TE_ITR recipe path:   {}\".format(get_te_itr_path(RECIPE_PATH_TE_ITR, 1, 1)))\n",
    "print(\"Sample TE_ITR model path:    {}\".format(get_te_itr_path(OS_MODEL_PATH_TE_ITR, 1, 1)))\n",
    "print(\"Sample UNET_ITR recipe path: {}\".format(get_unet_itr_path(RECIPE_PATH_UNET_ITR, 1)))\n",
    "print(\"Sample UNET_ITR model path:  {}\".format(get_unet_itr_path(OS_MODEL_PATH_UNET_ITR, 1)))\n",
    "print(\"Does RAW model exists:       {}\".format(os.path.isfile(OS_MODEL_PATH_RAW)))\n",
    "print(\"Final model path:            {}\".format(MECHA_MODEL_PATH_FINAL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick VAE ###\n",
    "- It will pick `cur_model` for every merge key and `vae_model` for every passthroguh key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_vae(cur_model):\n",
    "    vae_model =  \"{}{}\".format(DIR_RAW,MODEL_LIST_RAW[MODEL_SELECTION_VAE - 1]) \n",
    "    return sd_mecha.weighted_sum(vae_model, cur_model, alpha=1.0, dtype=g_precision_while_merge, device=g_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Average ###\n",
    "\n",
    "- Pay attention to the `alpha`. It is opposite to A1111: It is \"A merge to B\" instead of \"Merge A with B\".\n",
    "- Also the receipe is set of `RecipeNode` under [tree structure](https://en.wikipedia.org/wiki/Tree_(data_structure)). Therefore you can expect there are quite a lot of recursive stuffs (returning iteself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recipe_naive_merge():\n",
    "    models = list(map(lambda p: \"{}{}\".format(DIR_RAW,p),MODEL_LIST_RAW))\n",
    "\n",
    "    recipe = models[0]\n",
    "    for i, model in enumerate(models[1:], start=2):\n",
    "        recipe = sd_mecha.weighted_sum(model, recipe, alpha=(i-1)/i, dtype=g_precision_while_merge, device=g_device)\n",
    "\n",
    "    return pick_vae(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_rmk(rmk_raw(), make_recipe_naive_merge())\n",
    "sd_mecha.serialize_and_save(recipe_mapping[rmk_raw()], RECIPE_PATH_RAW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP Models to test ###\n",
    "- Note that `alpha` is using a special operator `|` which is \"Bitwise OR\".\n",
    "- Also recall \"TE0 use ViT-G\" and \"TE1 use ViT-L\" and \"TE2 use both\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recipe_te_itr(p):\n",
    "    clip_model = \"{}{}\".format(DIR_RAW, p)\n",
    "    unet_model = \"{}{}\".format(MECHA_MODEL_PATH_RAW, MECHA_MODEL_EXT)\n",
    "    recipe_te0 = sd_mecha.weighted_sum(clip_model, unet_model, alpha=(sd_mecha.sdxl_txt_classes(1.0) | sd_mecha.sdxl_txt_g14_classes(0.0) | sd_mecha.sdxl_unet_classes(1.0)), dtype=g_precision_while_merge, device=g_device)\n",
    "    recipe_te1 = sd_mecha.weighted_sum(clip_model, unet_model, alpha=(sd_mecha.sdxl_txt_classes(0.0) | sd_mecha.sdxl_txt_g14_classes(1.0) | sd_mecha.sdxl_unet_classes(1.0)), dtype=g_precision_while_merge, device=g_device)\n",
    "    recipe_te2 = sd_mecha.weighted_sum(clip_model, unet_model, alpha=(sd_mecha.sdxl_txt_classes(0.0) | sd_mecha.sdxl_txt_g14_classes(0.0) | sd_mecha.sdxl_unet_classes(1.0)), dtype=g_precision_while_merge, device=g_device)\n",
    "    return (pick_vae(recipe_te0), pick_vae(recipe_te1), pick_vae(recipe_te2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3N models\n",
    "for i in range(len(MODEL_LIST_RAW)):\n",
    "    rte = make_recipe_te_itr(MODEL_LIST_RAW[i])\n",
    "    for j in range(len(N_CLIP)):    \n",
    "        set_rmk(rmk_te(i+1,j), rte[j])\n",
    "        sd_mecha.serialize_and_save(recipe_mapping[rmk_te(i+1,j)], get_te_itr_path(RECIPE_PATH_TE_ITR,i,j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picked CLIP Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recipe_ste():\n",
    "    raw_models = list(map(lambda p: \"{}{}\".format(DIR_RAW,p),MODEL_LIST_RAW))\n",
    "    models_te0 = [raw_models[i-1] for i in MODEL_SELECTION_TE0]\n",
    "    models_te1 = [raw_models[i-1] for i in MODEL_SELECTION_TE1]\n",
    "\n",
    "    unet_model = \"{}{}\".format(MECHA_MODEL_PATH_RAW, MECHA_MODEL_EXT)\n",
    "\n",
    "    recipe_te0 = models_te0[0]\n",
    "    for i, model in enumerate(models_te0[1:], start=2):\n",
    "        recipe_te0 = sd_mecha.weighted_sum(model, recipe_te0, alpha=(i-1)/i, dtype=g_precision_while_merge, device=g_device)\n",
    "    recipe_te1 = models_te1[0]\n",
    "    for i, model in enumerate(models_te1[1:], start=2):\n",
    "        recipe_te1 = sd_mecha.weighted_sum(model, recipe_te1, alpha=(i-1)/i, dtype=g_precision_while_merge, device=g_device)\n",
    "\n",
    "    unet_model = sd_mecha.weighted_sum(recipe_te0, unet_model, alpha=(sd_mecha.sdxl_txt_classes(1.0) | sd_mecha.sdxl_txt_g14_classes(0.0) | sd_mecha.sdxl_unet_classes(1.0)), dtype=g_precision_while_merge, device=g_device)\n",
    "    unet_model = sd_mecha.weighted_sum(recipe_te1, unet_model, alpha=(sd_mecha.sdxl_txt_classes(0.0) | sd_mecha.sdxl_txt_g14_classes(1.0) | sd_mecha.sdxl_unet_classes(1.0)), dtype=g_precision_while_merge, device=g_device)\n",
    "\n",
    "    return pick_vae(unet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_rmk(rmk_ste(), make_recipe_ste())\n",
    "sd_mecha.serialize_and_save(recipe_mapping[rmk_ste()], RECIPE_PATH_SELECTED_TE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNET Models to test ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recipe_unet_itr(p):\n",
    "    unet_model = \"{}{}\".format(DIR_RAW, p)\n",
    "    clip_model = \"{}{}\".format(MECHA_MODEL_PATH_SELECTED_TE, MECHA_MODEL_EXT)\n",
    "    recipe_te2 = sd_mecha.weighted_sum(clip_model, unet_model, alpha=(sd_mecha.sdxl_txt_classes(0.0) | sd_mecha.sdxl_txt_g14_classes(0.0) | sd_mecha.sdxl_unet_classes(1.0)), dtype=g_precision_while_merge, device=g_device)\n",
    "    return pick_vae(recipe_te2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N models\n",
    "for i in range(len(MODEL_LIST_RAW)):\n",
    "    set_rmk(rmk_unet(i+1), make_recipe_unet_itr(MODEL_LIST_RAW[i]))\n",
    "    sd_mecha.serialize_and_save(recipe_mapping[rmk_unet(i+1)], get_unet_itr_path(RECIPE_PATH_UNET_ITR,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model ###\n",
    "\n",
    "- 2 models will be produced for validation. The real e2e and staged merging should yield same model within floating errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recipe_final():\n",
    "    raw_models = list(map(lambda p: \"{}{}\".format(DIR_RAW,p),MODEL_LIST_RAW))\n",
    "    models_unet = [raw_models[i-1] for i in MODEL_SELECTION_UNET]\n",
    "\n",
    "    clip_model = \"{}{}\".format(MECHA_MODEL_PATH_SELECTED_TE, MECHA_MODEL_EXT)\n",
    "\n",
    "    recipe_unet = models_unet[0]\n",
    "    for i, model in enumerate(models_unet[1:], start=2):\n",
    "        recipe_unet = sd_mecha.weighted_sum(model, recipe_unet, alpha=(i-1)/i, dtype=g_precision_while_merge, device=g_device)\n",
    "\n",
    "    final_model = sd_mecha.weighted_sum(clip_model, recipe_unet, alpha=(sd_mecha.sdxl_txt_classes(0.0) | sd_mecha.sdxl_txt_g14_classes(0.0) | sd_mecha.sdxl_unet_classes(1.0)), dtype=g_precision_while_merge, device=g_device)\n",
    "\n",
    "    return pick_vae(final_model)\n",
    "\n",
    "def make_recipe_e2e():\n",
    "    raw_models = list(map(lambda p: \"{}{}\".format(DIR_RAW,p),MODEL_LIST_RAW))\n",
    "    models_te0 = [raw_models[i-1] for i in MODEL_SELECTION_TE0]\n",
    "    models_te1 = [raw_models[i-1] for i in MODEL_SELECTION_TE1]\n",
    "    models_unet = [raw_models[i-1] for i in MODEL_SELECTION_UNET]\n",
    "\n",
    "    recipe_te0 = models_te0[0]\n",
    "    for i, model in enumerate(models_te0[1:], start=2):\n",
    "        recipe_te0 = sd_mecha.weighted_sum(model, recipe_te0, alpha=(i-1)/i, dtype=g_precision_while_merge, device=g_device)\n",
    "    recipe_te1 = models_te1[0]\n",
    "    for i, model in enumerate(models_te1[1:], start=2):\n",
    "        recipe_te1 = sd_mecha.weighted_sum(model, recipe_te1, alpha=(i-1)/i, dtype=g_precision_while_merge, device=g_device)\n",
    "    recipe_unet = models_unet[0]\n",
    "    for i, model in enumerate(models_unet[1:], start=2):\n",
    "        recipe_unet = sd_mecha.weighted_sum(model, recipe_unet, alpha=(i-1)/i, dtype=g_precision_while_merge, device=g_device)\n",
    "\n",
    "    e2e_model = \"{}{}\".format(DIR_RAW,MODEL_LIST_RAW[MODEL_SELECTION_VAE - 1]) \n",
    "\n",
    "    e2e_model = sd_mecha.weighted_sum(e2e_model, recipe_unet, alpha=(sd_mecha.sdxl_txt_classes(0.0) | sd_mecha.sdxl_txt_g14_classes(0.0) | sd_mecha.sdxl_unet_classes(1.0)), dtype=g_precision_while_merge, device=g_device)\n",
    "    e2e_model = sd_mecha.weighted_sum(e2e_model, recipe_te0, alpha=(sd_mecha.sdxl_txt_classes(0.0) | sd_mecha.sdxl_txt_g14_classes(1.0) | sd_mecha.sdxl_unet_classes(0.0)), dtype=g_precision_while_merge, device=g_device)\n",
    "    e2e_model = sd_mecha.weighted_sum(e2e_model, recipe_te1, alpha=(sd_mecha.sdxl_txt_classes(1.0) | sd_mecha.sdxl_txt_g14_classes(0.0) | sd_mecha.sdxl_unet_classes(0.0)), dtype=g_precision_while_merge, device=g_device)\n",
    "\n",
    "    #Note that there is no pick_vae\n",
    "    return e2e_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_rmk(rmk_f(), make_recipe_final())\n",
    "sd_mecha.serialize_and_save(recipe_mapping[rmk_f()], RECIPE_PATH_FINAL)\n",
    "set_rmk(rmk_e2e(), make_recipe_e2e())\n",
    "sd_mecha.serialize_and_save(recipe_mapping[rmk_e2e()], RECIPE_PATH_E2E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for action ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Average ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model is present. Skipping.\n",
      "Merge time: 0 sec\n"
     ]
    }
   ],
   "source": [
    "tss = time.time()\n",
    "\n",
    "if MODE_RAW in MODE_ACTIVATED:\n",
    "    if os.path.isfile(OS_MODEL_PATH_RAW):\n",
    "        print(\"Merged model is present. Skipping.\")\n",
    "    else:\n",
    "        scheduler.merge_and_save(recipe_mapping[rmk_raw()], output_path=MECHA_MODEL_PATH_RAW, save_dtype=g_precision_final_model, total_buffer_size=g_total_buffer_size)\n",
    "else:\n",
    "    print(\"This session is not activated. Skipping.\")\n",
    "\n",
    "tse = time.time()\n",
    "print(\"Merge time: {} sec\".format(int(tse - tss)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP Models to test ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merge time: 0 sec\n"
     ]
    }
   ],
   "source": [
    "tss = time.time()\n",
    "\n",
    "if MODE_CLIP in MODE_ACTIVATED:\n",
    "    # 3N models\n",
    "    #for i in tqdm(range(len(MODEL_LIST_RAW)), desc=\"Iterlating raw model list to swap TEs\"):\n",
    "    #    for j in tqdm(range(len(N_CLIP)), desc=\"Making models with swapped raw TEs\"):\n",
    "    for i in range(len(MODEL_LIST_RAW)):\n",
    "        for j in range(len(N_CLIP)):    \n",
    "            if os.path.isfile(get_te_itr_path(OS_MODEL_PATH_TE_ITR,i,j)):\n",
    "                print(\"Merged model is present. Skipping.\")\n",
    "            else:\n",
    "                scheduler.merge_and_save(recipe_mapping[rmk_te(i+1,j)], output_path=get_te_itr_path(MECHA_MODEL_PATH_TE_ITR,i,j), save_dtype=g_precision_final_model, total_buffer_size=g_total_buffer_size)\n",
    "else:\n",
    "    print(\"This session is not activated. Skipping.\")\n",
    "\n",
    "tse = time.time()\n",
    "print(\"Merge time: {} sec\".format(int(tse - tss)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picked CLIP Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model is present. Skipping.\n",
      "Merge time: 0 sec\n"
     ]
    }
   ],
   "source": [
    "tss = time.time()\n",
    "\n",
    "if MODE_CLIP in MODE_ACTIVATED:\n",
    "    if os.path.isfile(OS_MODEL_PATH_SELECTED_TE):\n",
    "        print(\"Merged model is present. Skipping.\")\n",
    "    else:\n",
    "        scheduler.merge_and_save(recipe_mapping[rmk_ste()], output_path=MECHA_MODEL_PATH_SELECTED_TE, save_dtype=g_precision_final_model, total_buffer_size=g_total_buffer_size)\n",
    "else:\n",
    "    print(\"This session is not activated. Skipping.\")\n",
    "\n",
    "tse = time.time()\n",
    "print(\"Merge time: {} sec\".format(int(tse - tss)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNET Models to test ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merged model is present. Skipping.\n",
      "Merge time: 0 sec\n"
     ]
    }
   ],
   "source": [
    "tss = time.time()\n",
    "\n",
    "if MODE_UNET in MODE_ACTIVATED:\n",
    "    # N models\n",
    "    #for i in tqdm(range(len(MODEL_LIST_RAW)), desc=\"Iterlating raw model list to swap UNETs\"):\n",
    "    for i in range(len(MODEL_LIST_RAW)):\n",
    "        if os.path.isfile(get_unet_itr_path(OS_MODEL_PATH_UNET_ITR,i)):\n",
    "            print(\"Merged model is present. Skipping.\")\n",
    "        else:\n",
    "            scheduler.merge_and_save(recipe_mapping[rmk_unet(i+1)], output_path=get_unet_itr_path(MECHA_MODEL_PATH_UNET_ITR,i), save_dtype=g_precision_final_model, total_buffer_size=g_total_buffer_size)\n",
    "else:\n",
    "    print(\"This session is not activated. Skipping.\")\n",
    "\n",
    "tse = time.time()\n",
    "print(\"Merge time: {} sec\".format(int(tse - tss)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./x51a-AstolfoMix-x39te0x39te1-240222-60d0764 ./x45-AstolfoMix-x39te0x39te1-240222-60d0764\n"
     ]
    }
   ],
   "source": [
    "print (MECHA_MODEL_PATH_SELECTED_TE, MECHA_MODEL_PATH_FINAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model is present. Skipping.\n",
      "Merge time: 0 sec\n"
     ]
    }
   ],
   "source": [
    "tss = time.time()\n",
    "\n",
    "if MODE_FINAL in MODE_ACTIVATED:\n",
    "    if os.path.isfile(OS_MODEL_PATH_FINAL):\n",
    "        print(\"Merged model is present. Skipping.\")\n",
    "    else:\n",
    "        scheduler.merge_and_save(recipe_mapping[rmk_f()], output_path=MECHA_MODEL_PATH_FINAL, save_dtype=g_precision_final_model, total_buffer_size=g_total_buffer_size)\n",
    "else:\n",
    "    print(\"This session is not activated. Skipping.\")\n",
    "\n",
    "tse = time.time()\n",
    "print(\"Merge time: {} sec\".format(int(tse - tss)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model is present. Skipping.\n",
      "Merge time: 0 sec\n"
     ]
    }
   ],
   "source": [
    "tss = time.time()\n",
    "\n",
    "if MODE_FINAL in MODE_ACTIVATED:\n",
    "    if os.path.isfile(OS_MODEL_PATH_E2E):\n",
    "        print(\"Merged model is present. Skipping.\")\n",
    "    else:\n",
    "        scheduler.merge_and_save(recipe_mapping[rmk_e2e()], output_path=MECHA_MODEL_PATH_E2E, save_dtype=g_precision_final_model, total_buffer_size=g_total_buffer_size)\n",
    "else:\n",
    "    print(\"This session is not activated. Skipping.\")\n",
    "\n",
    "tse = time.time()\n",
    "print(\"Merge time: {} sec\".format(int(tse - tss)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full operation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0 sec\n"
     ]
    }
   ],
   "source": [
    "te = time.time()\n",
    "print(\"Total time: {} sec\".format(int(te - ts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
