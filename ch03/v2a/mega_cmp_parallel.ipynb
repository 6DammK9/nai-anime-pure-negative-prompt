{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "019fd4e3-4c88-43a3-a2bb-3ec4c7d0fa92",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Mega similarity check (V2b) #\n",
    "\n",
    "### Abstract ###\n",
    "\n",
    "- Distance metric is not meaningful, **but the identity does.**\n",
    "- However I still prefer L2 distance ~~because it looks like I'm doing a ML task~~.\n",
    "- Objective: Try to explain *subjective* experience with model difference, especially if any components have been **changed**, ignoring how much it has been changed.\n",
    "- Thanks [\"CC\"](https://github.com/crosstyan), [\"RC\"](https://github.com/CCRcmcpe) and [\"AO\"](https://github.com/AdjointOperator) for providing the initial script (and the idea).\n",
    "\n",
    "### Changelog ###\n",
    "\n",
    "- `v2b`: Apply [concurrent feature](https://stackoverflow.com/questions/51601756/use-tqdm-with-concurrent-futures) to speed up the process. However it will be cpu only.\n",
    "- `v2`: Scan a folder in a static type (SD 1.x) and path, then try to build a \"distance matrix\" and visualize it.\n",
    "- `v1`: Custom model group / paths. Can compare different models by interest but features are limited.\n",
    "\n",
    "### Required libraries ###\n",
    "\n",
    "- ~~Should be the common ML pack we're using. Also with [SD webui's dependency](https://github.com/AUTOMATIC1111/stable-diffusion-webui).~~\n",
    "\n",
    "- [scikit-learn](https://scikit-learn.org/stable/install.html)\n",
    "- [NetworkX](https://networkx.org/documentation/stable/release/release_3.0.html)\n",
    "- [safetensors](https://huggingface.co/docs/safetensors/index)\n",
    "- [pytorch](https://pytorch.org/get-started/locally/#windows-python)\n",
    "- [matplotlib](https://matplotlib.org/stable/api/matplotlib_configuration_api.html)\n",
    "- [numpy](https://numpy.org/)\n",
    "\n",
    "### Input ### \n",
    "- See next cell. Paths of models and abbreviation you like.\n",
    "\n",
    "### Output ###\n",
    "- TONS of JSON, showing `(layer_name, distance_between_2_models)`\n",
    "- TONS of IMG, showing `(pair_of_model, distance_for_each_type_of_diffusion_layer)`\n",
    "\n",
    "### Special case or comparasion ###\n",
    "- Text encoder for model `nai`: `\"cond_stage_model.transformer\", \"cond_stage_model.transformer.text_model\"`\n",
    "\n",
    "### Some layer name to interprept ###\n",
    "\n",
    "- Whole model combined as called `DiffusionPipeline` in [Diffusers](https://huggingface.co/docs/diffusers/index).\n",
    "\n",
    "|Layer name|Description|Class name in Diffusers|\n",
    "|---|---|---|\n",
    "|`first_stage_model`|VAE|`AutoencoderKL`|\n",
    "|`cond_stage_model`|Text Encoder (SD1, SD2)|`CLIPTextModel`|\n",
    "|`conditioner.embedders.0`|Text Encoder 1 (SDXL)|`CLIPTextModel`|\n",
    "|`conditioner.embedders.1`|Text Encoder 2 (SDXL)|`CLIPTextModelWithProjection`|\n",
    "|`model.diffusion_model`|UNET|`UNet2DConditionModel`|\n",
    "|`model_ema`|EMA model for training|n/a|\n",
    "|`cumprod`, `betas`, `alphas`|`CosineAnnealingLR`|n/a|\n",
    "\n",
    "### Some notation (Useful in the bin chart) ###\n",
    "- `attn1`: `sattn` = *Self attention*\n",
    "- `attn2`: `xattn` = *Cross attention*\n",
    "- `ff`: *Feed forward*\n",
    "- `norm`: [Normalisation layer](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html). `elementwise_affine=True` introduces trainable `bias` and `weight`. \n",
    "- `proj`: *Projection*\n",
    "- `emb_layers`: *Embedding layers*\n",
    "- `mlp`: *Multilayer perceptron*\n",
    "- `others`: `ff` + `norm` + `proj` + `emb_layers`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c8a246d",
   "metadata": {},
   "source": [
    "Path configuration. Make sure the model names are named with ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6714cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths here.\n",
    "cmp_keyword = \"dgmla3\"\n",
    "\n",
    "ofp_folder = {\n",
    "    \"json\": \"./json_v2_{}/\".format(cmp_keyword),\n",
    "    \"img\": \"./img_v2_{}/\".format(cmp_keyword)\n",
    "}\n",
    "\n",
    "# Model group folder\n",
    "model_folder = \"F:/NOVELAI/astolfo_mix/sdxl/raw/\" #\"../../stable-diffusion-webui/tmp/astolfo_mix/sdxl/raw/\"\n",
    "\n",
    "# Model group key (see v1 for example)\n",
    "model_group_key = cmp_keyword\n",
    "\n",
    "# Note: this is a output file instead of input.\n",
    "model_map_o = \"./json_v2_{}/model_map_v2.json\".format(cmp_keyword)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "413f25c8",
   "metadata": {},
   "source": [
    "- `am0`: Merge process of [AstolfoMix](../../ch05/README.md). No CLIP / TE reset.\n",
    "- `am1`: Merge process of [AstolfoMix](../../ch05/README.md). With CLIP / TE reset.\n",
    "- `am2`: Receipe model of [AstolfoMix](../../ch05/README.md).\n",
    "- `am3`: Merge process of [AstolfoMix](../../ch05/README.md). Extended from `am0`\n",
    "- `am4`: Receipe model of [AstolfoMix](../../ch05/README.md). Extended from `am2`\n",
    "- `am5`: Merge process of [AstolfoMix](../../ch05/README.md). `am0` with `am3`.\n",
    "- `am6`: Receipe model (UNET) of [AstolfoMix-SD2](../../ch05/README_SD2.md).\n",
    "- `am7`: `am6` but removed 2 outliers.\n",
    "- `am7b`: Receipe model (TE) of [AstolfoMix-SD2](../../ch05/README_SD2.md).\n",
    "- `am8`: Merge process of [AstolfoMix-SD2](../../ch05/README_SD2.md)(209b).\n",
    "- `am9`: Merge process of [AstolfoMix-SD2](../../ch05/README_SD2.md)(210b).\n",
    "- `21b`: Receipe model of [AstolfoMix \"21b\"](../../ch05/README.md).\n",
    "- `dgmla2b`: Recent merges of [AstolfoMix SDXL DGMLA-216](../../ch05/README_XL.md)(116a to 215a).\n",
    "- `main0`: Same as `main`, but with [dreamlike-art/dreamlike-photoreal-2.0](https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0) as an outlier.\n",
    "- `main`: All the models below, but seleting the representive of the models.\n",
    "- `cwd`: Dataset for [ThePioneer/CoolerWaifuDiffusion](https://huggingface.co/ThePioneer/CoolerWaifuDiffusion).\n",
    "- `sd2`: All SD 2.x based models.\n",
    "- `sdxl0`: All SDXL based models (10 out of 40 includes original SDXL 1.0).\n",
    "- `sdxl1`: All SDXL based models (10 out of 40 includes original SDXL 1.0).\n",
    "- `sdxl2`: All SDXL based models (10 out of 40 includes original SDXL 1.0).\n",
    "- `sdxl3`: All SDXL based models (10 out of 40 includes original SDXL 1.0).\n",
    "- `sdxl4`: All SDXL based models (handpicked 10 outliers includes original SDXL 1.0).\n",
    "- `sdxl5`: All SDXL based models (10 out of 52 includes original SDXL 1.0).\n",
    "- `sdxl6`: All SDXL based models (52 out of 52 includes original SDXL 1.0). Done in a 512GB RAM Workstation.\n",
    "- `pony1`: All SDXL based model (118 models). Workstation upgraded to 4TB.  **Should use around 780GB of RAM.**\n",
    "- `dgmla3`: All SDXL based model (220 models), with `dgmla2b` (12 models). **Should use around 1.5TB of RAM.**\n",
    "- `wdacnai0`: Same as `wdacnai`, but with SD 1.2 and 1.5 also.\n",
    "- `wdacnai`: Dataset for [hakurei/waifu-diffusion-v1-3](https://huggingface.co/hakurei/waifu-diffusion-v1-3), [JosephusCheung/ACertainty](https://huggingface.co/JosephusCheung/ACertainty) and [CompVis/stable-diffusion-v-1-4-original](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original)\n",
    "- `evt`: Dataset for [haor/Evt_V4-preview](https://huggingface.co/haor/Evt_V4-preview)\n",
    "- `merge`: Dataset for a brunch of merged models with their believed recipes.\n",
    "- `any3`: Dataset for [AdamOswald1/anything-v5.0](https://huggingface.co/AdamOswald1/anything-v5.0).\n",
    "- `nnai`: Dataset for [JosephusCheung/ACertainty](https://huggingface.co/JosephusCheung/ACertainty).\n",
    "- `aocc`: Dataset for [AnnihilationOperator/ABPModel](https://huggingface.co/AnnihilationOperator/ABPModel) vs  [Crosstyan/BPModel](https://huggingface.co/Crosstyan/BPModel)\n",
    "- `aobp`: Dataset for [AnnihilationOperator/ABPModel](https://huggingface.co/AnnihilationOperator/ABPModel)\n",
    "- `ccbp`: Dataset for [Crosstyan/BPModel](https://huggingface.co/Crosstyan/BPModel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdfa8987",
   "metadata": {},
   "source": [
    "Load libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6517a1-1515-415c-819d-5b496e5676e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import torch\n",
    "from safetensors.torch import load_file #safe_open\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib.concurrent import thread_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d65f468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0+cu124'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0eab566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for OMP: Error #15\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75c910ff",
   "metadata": {},
   "source": [
    "Some operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300ccc5b-4363-4002-8018-30e72fd0fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Support 'cuda', but 'cpu' is arleady fast.\n",
    "g_device = \"cpu\"\n",
    "# Currently for generating graph only.\n",
    "g_seed = 114514\n",
    "# How many threads for this process\n",
    "g_threads = 48 if g_device == \"cpu\" else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "385760de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output folder\n",
    "for v in ofp_folder.values():\n",
    "    os.makedirs(os.path.dirname(v), exist_ok=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a063cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = os.listdir(model_folder)\n",
    "# Exclude yaml.\n",
    "model_list = list(filter(lambda p: p.endswith(\".ckpt\") or p.endswith(\".safetensors\") or p.endswith(\".bin\"), model_list))\n",
    "if len(model_list) < 2:\n",
    "    raise Exception(\"Need at least 2 models for comparasion.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6566ce0a",
   "metadata": {},
   "source": [
    "Expected $O(N^2)$ for [pairwise distances for distance matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). For example, $6*5/2 = 15$ pairs for 6 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40a4e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cmp_mapping(models):\n",
    "    i = 0\n",
    "    cm = [[model_group_key, []]]\n",
    "    for i0 in range(len(model_list)):        \n",
    "        for i1 in range(i0 + 1, len(model_list)):\n",
    "            i = i + 1\n",
    "            # Do not modify the model key: it is used to build the distance matrix! \n",
    "            cm[0][1].append([\n",
    "                [str(\"m{}\".format(i0)), \"{}{}\".format(model_folder, model_list[i0])],\n",
    "                [str(\"m{}\".format(i1)), \"{}{}\".format(model_folder, model_list[i1])],\n",
    "            ])\n",
    "    print(\"{} model pairs to go.\".format(i))\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "773c102c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26796 model pairs to go.\n"
     ]
    }
   ],
   "source": [
    "cmp_mapping = make_cmp_mapping(model_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d3f3d4b",
   "metadata": {},
   "source": [
    "Dump result first. Will dump the distance matrix later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e8d031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmj = {\n",
    "    \"ml\": model_list,\n",
    "    \"cm\": cmp_mapping\n",
    "}\n",
    "with open(model_map_o, \"w\") as fj:\n",
    "    json.dump(cmj, fj, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6973fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 only. I don't expect to handcraft the mapping here.\n",
    "if False:\n",
    "    # For \"micro_cmp\", go to the cell with cmp_json()\n",
    "    cmp_mapping = []\n",
    "    try:\n",
    "        with open(model_map_o, \"r\") as mmf:\n",
    "            read_content = mmf.read()\n",
    "            cmp_mapping = json.loads(read_content)\n",
    "    except:\n",
    "        print(\"Error when loading model map. There won't be mass scale comparasion.\".format(model_map_o))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff05df5c",
   "metadata": {},
   "source": [
    "Distance matrix for model components. Default zero (identical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa2b9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_dm = {\n",
    "    \"te\": torch.zeros((len(model_list), len(model_list))),\n",
    "    \"te0\": torch.zeros((len(model_list), len(model_list))),\n",
    "    \"te1\": torch.zeros((len(model_list), len(model_list))),\n",
    "    \"vae\": torch.zeros((len(model_list), len(model_list))),\n",
    "    \"unet\": torch.zeros((len(model_list), len(model_list))),\n",
    "    \"other\": torch.zeros((len(model_list), len(model_list)))\n",
    "}\n",
    "\n",
    "model_cache = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2610e9f",
   "metadata": {},
   "source": [
    "Functions inside the compare loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8ee09f7-dcac-4e7f-908e-5fb14bdd9cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path: Path, device: str, print_ptl_info=False) -> dict[str, torch.Tensor]:\n",
    "    if \".safetensors\" in path.suffixes:\n",
    "        return load_file(path, device=device)\n",
    "    else:\n",
    "        ckpt = torch.load(path, map_location=device)\n",
    "        if print_ptl_info and \"epoch\" in ckpt and \"global_step\" in ckpt:\n",
    "            print(f\"[I] {path.name}: epoch {ckpt['epoch']}, step {ckpt['global_step']}\")\n",
    "        return ckpt[\"state_dict\"] if \"state_dict\" in ckpt else ckpt\n",
    "\n",
    "# Reminder: Dodge different shape!\n",
    "def check_equal_shape(a: torch.Tensor, b: torch.Tensor, fn):\n",
    "    if a.shape != b.shape:\n",
    "        raise Exception(\"DIFFERENT SHAPE\")\n",
    "        #print(\"DIFFERENT SHAPE: return -1.0\")\n",
    "        #return -1.0\n",
    "    return fn(a.type(torch.float),b.type(torch.float))\n",
    "\n",
    "TENSOR_METRIC_MAP = {\n",
    "    #\"equal\": torch.equal,\n",
    "    \"l0\": lambda a, b: check_equal_shape(a, b, lambda a, b: torch.dist(a, b, p=0)),    \n",
    "    \"l1\": lambda a, b: check_equal_shape(a, b, lambda a, b: torch.dist(a, b, p=1)),\n",
    "    \"l2\": lambda a, b: check_equal_shape(a, b, lambda a, b: torch.dist(a, b, p=2)),\n",
    "    \"cossim\": lambda a, b: check_equal_shape(a, b, lambda a, b: torch.mean(torch.cosine_similarity(a, b, dim=0)))\n",
    "}\n",
    "\n",
    "FIG_METRIC_MAP = {\n",
    "    #\"equal\": lambda v: np.linalg.norm(v, 0), \n",
    "    \"l0\": lambda v: torch.linalg.norm(v, 0),    \n",
    "    \"l1\": lambda v: torch.linalg.norm(v, 1),\n",
    "    \"l2\": lambda v: torch.linalg.norm(v, 2),\n",
    "    #I don't know how to make this meaningful...\n",
    "    \"cossim\": lambda v: torch.linalg.norm(v, None)\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6da9388",
   "metadata": {},
   "source": [
    "Read a pair of models, extract the key paths, compare for difference, and return all the intermediate data (useful for next step).\n",
    "\n",
    "In graphical sense: `(da(kv)ab)err`. Obvious?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ef0b25b-61e4-4bfb-8bcc-cbbf8e148753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substring_in_d(k, da):\n",
    "    for d in da:\n",
    "        if k in d:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def cmp_c(a_path, b_path, device, metric, no_ptl_info):\n",
    "    metric_fn = TENSOR_METRIC_MAP[metric]\n",
    "    \n",
    "    try:\n",
    "        a_path = a_path.decode('UTF-8')\n",
    "        b_path = b_path.decode('UTF-8')\n",
    "    except:\n",
    "        #No need\n",
    "        pass\n",
    "\n",
    "    # Load from model cache\n",
    "    if a_path not in model_cache:\n",
    "        model_cache[a_path] = load_model(Path(a_path), device, not no_ptl_info)\n",
    "    if b_path not in model_cache:\n",
    "        model_cache[b_path] = load_model(Path(b_path), device, not no_ptl_info)\n",
    "    \n",
    "    a = model_cache[a_path] \n",
    "    b = model_cache[b_path]\n",
    "\n",
    "    ak = set(a.keys())\n",
    "    bk = set(b.keys())\n",
    "    \n",
    "    keys_inter = ak.intersection(bk)\n",
    "    da = list(ak.difference(bk))\n",
    "    db = list(bk.difference(ak))\n",
    "    kv = {}\n",
    "    err = []\n",
    "    for k in keys_inter:\n",
    "        try:\n",
    "            rt = metric_fn(a[k], b[k])\n",
    "            #rt = rt.numpy().tolist()\n",
    "            # Wow infinity.\n",
    "            if math.isinf(rt):\n",
    "                raise Exception(\"Infinity is found.\")\n",
    "            if math.isnan(rt):\n",
    "                raise Exception(\"NaN is found.\")\n",
    "            kv[k] = rt\n",
    "        except:\n",
    "            #Silenced.\n",
    "            #\"nan\" or True / False\n",
    "            #print(\"DIFFERENT SHAPE at key {}. Ignored.\".format(k))\n",
    "            err.append(k)\n",
    "            pass        \n",
    "\n",
    "    #Special case: NAI renamed the TE (claimed using GPT-2)\n",
    "    #Another special case: Somehow it still shows no key...\n",
    "    if not ((\"animefull\" in str(a_path)) and (\"animefull\" in str(b_path))):\n",
    "        if \"animefull\" in str(a_path):\n",
    "            nda = substring_in_d(\"cond_stage_model.transformer.text_model\", db)\n",
    "            for dak in da:\n",
    "                if (\"cond_stage_model.transformer\" in dak) and nda:\n",
    "                    try:\n",
    "                        kv[\"nai.\" + dak] = metric_fn(a[dak], b[dak.replace(\"cond_stage_model.transformer\", \"cond_stage_model.transformer.text_model\")]) #.numpy().tolist()\n",
    "                    except:\n",
    "                        err.append(dak)\n",
    "        elif \"animefull\" in str(b_path):\n",
    "            ndb = substring_in_d(\"cond_stage_model.transformer.text_model\", da)\n",
    "            for dbk in db:\n",
    "                if (\"cond_stage_model.transformer\" in dbk) and ndb:\n",
    "                    try:\n",
    "                        kv[\"nai.\" + dbk] = metric_fn(b[dbk], a[dbk.replace(\"cond_stage_model.transformer\", \"cond_stage_model.transformer.text_model\")]) #.numpy().tolist()\n",
    "                    except:\n",
    "                        err.append(dbk)\n",
    "    \n",
    "    return kv, da, db, err, a, b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d7262b9",
   "metadata": {},
   "source": [
    "Plot graph from the results above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e76a270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/python-extract-numbers-from-string/\n",
    "def index_from_model_path_key(txt):\n",
    "    temp = re.findall(r'\\d+', txt)\n",
    "    res = list(map(int, temp))\n",
    "    return res[0]\n",
    "\n",
    "def cmp_attn(pak, pbk, kv, a, b, ofi, d):\n",
    "    dmia = index_from_model_path_key(pak)\n",
    "    dmib = index_from_model_path_key(pbk)\n",
    "    tmfn = TENSOR_METRIC_MAP[d]\n",
    "    fmfn = FIG_METRIC_MAP[d]\n",
    "    diffs = {}\n",
    "    dlabel = d.upper() #L2\n",
    "\n",
    "    no_unet = True\n",
    "\n",
    "    # Ensure there is UNET.\n",
    "    for k in kv.keys():\n",
    "        if k.startswith('model.diffusion_model'):\n",
    "            no_unet = False\n",
    "            break\n",
    "\n",
    "    if no_unet:\n",
    "        return\n",
    "\n",
    "    for k in kv.keys():\n",
    "        #This time I look for components instead of layers        \n",
    "        #But I must dodge NAI renamed TE (identical to SD 1.4 so do it seperately if needed)\n",
    "        if k.startswith('nai.cond_stage_model'):\n",
    "            continue\n",
    "        \n",
    "        delta = tmfn(a[k], b[k]) #.numpy().tolist()\n",
    "     \n",
    "        if k.startswith('cond_stage_model'):\n",
    "            c = 'te'\n",
    "        elif k.startswith('conditioner.embedders.0'):\n",
    "            c = 'te0'\n",
    "        elif k.startswith('conditioner.embedders.1'):\n",
    "            c = 'te1'\n",
    "        elif k.startswith('first_stage_model'):\n",
    "            c = 'vae'\n",
    "        elif k.startswith('model.diffusion_model'):\n",
    "            c = 'unet'\n",
    "        else:\n",
    "            c = 'other'            \n",
    "        diffs.setdefault(c, []).append(delta)\n",
    "\n",
    "    for k in diffs:\n",
    "        #diffs[k] = np.concatenate([diffs[k]], axis=0)\n",
    "        diffs[k] = torch.cat((torch.tensor([diffs[k]]),), 0) #float\n",
    "\n",
    "    lsp = len(diffs.keys())\n",
    "    if (lsp == 0):\n",
    "        raise Exception(\"Why others is absent?\")\n",
    "\n",
    "    # 240611: Too many images.\n",
    "    if False:\n",
    "        fig, axs = plt.subplots(lsp, 1, figsize=(16, lsp * 4), sharex=False)\n",
    "        fig.tight_layout(pad=4.0)\n",
    "        for i, (k, v) in enumerate(diffs.items()):\n",
    "            #bins=len(v) for finding outliers\n",
    "            #v: numpy.array. 80 layers for attn, 526 for others.\n",
    "            dval = fmfn(v)\n",
    "\n",
    "            # Note: In general, a distance matrix is a weighted adjacency matrix of some graph. (wiki)\n",
    "            cmp_dm[k][dmia][dmib] = dval\n",
    "            cmp_dm[k][dmib][dmia] = dval\n",
    "\n",
    "            # I hate this bug\n",
    "            aaxs = axs if lsp == 1 else axs[i]\n",
    "            aaxs.hist(v, bins=len(v), density=False)\n",
    "            aaxs.set(xlabel=dlabel, ylabel='a.u.')\n",
    "            aaxs.xaxis.labelpad = 20\n",
    "            aaxs.set_yscale('log')\n",
    "            aaxs.set_title(f'{k}: ${dlabel}={dval:.4f}$')\n",
    "        \n",
    "        plt.savefig(ofi, bbox_inches='tight')\n",
    "        #WTF the plot retains? Why?\n",
    "        plt.close()\n",
    "\n",
    "    for i, (k, v) in enumerate(diffs.items()):\n",
    "        #bins=len(v) for finding outliers\n",
    "        #v: numpy.array. 80 layers for attn, 526 for others.\n",
    "        dval = fmfn(v)\n",
    "\n",
    "        # Note: In general, a distance matrix is a weighted adjacency matrix of some graph. (wiki)\n",
    "        cmp_dm[k][dmia][dmib] = dval\n",
    "        cmp_dm[k][dmib][dmia] = dval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bd474ec",
   "metadata": {},
   "source": [
    "Procedure of a comparasion. Original scripts has [custom garbage collection](https://docs.python.org/3/library/gc.html), but its default setting is fine for me. Also $O(N^2)$ comparasion is harsh.\n",
    "\n",
    "Variables explanation for \"nice guys\":\n",
    "\n",
    "|var|text|\n",
    "|---|---|\n",
    "|pak|Index of model A in model list.|\n",
    "|pbk|Index of model B in model list.|\n",
    "|pav|Path of model A.|\n",
    "|pbv|Path of model B.|\n",
    "|ofp|Folder path for output JSON reports.|\n",
    "|ofi|Folder path for output PNG plots.|\n",
    "|d|Distancing method, [p-norm](https://en.wikipedia.org/wiki/Norm_(mathematics)) or [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity).[pytorch](https://pytorch.org/docs/stable/generated/torch.dist.html), [numpy](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)|\n",
    "|npi|`no_ptl_info`. IDK what it means.|\n",
    "|kv|[Key-Value Pairs of intersection.](https://www.w3schools.com/js/js_json_objects.asp)|\n",
    "|da|Distinct content of model A.|\n",
    "|db|Distinct content of model B.|\n",
    "|err|Interset layers which throw errors. Usually they're in different shape.|\n",
    "|a|Instance of model A.|\n",
    "|b|Instance of model B.|\n",
    "|dj|Data for output JSON file.|\n",
    "|fj|File path for output JSON file.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cc121bc-e1c9-4e0a-84b9-ad2e50eb4cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 240611: Too many files.\n",
    "def cmp_json(pak, pbk, pav, pbv, ofp, ofi, d, npi):\n",
    "    kv, da, db, err, a, b = cmp_c(Path(pav), Path(pbv), g_device, d, npi)\n",
    "    dj = {'kv':kv, 'da':da, 'db':db, 'err': err}\n",
    "    #with open(ofp, \"w\") as fj:\n",
    "    #    json.dump(dj, fj, indent=4, sort_keys=True)\n",
    "    cmp_attn(pak, pbk, kv, a, b, ofi, d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "150cc0fa",
   "metadata": {},
   "source": [
    "Test / Manual operation for a single comparasion.\n",
    "\n",
    "~~Also as example for the above variables.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55067342-8567-472c-9a07-dbbb4b210c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nothing to test here. \n",
    "if False:\n",
    "    # Testing: Obvious result\n",
    "    cmp_json(\n",
    "        \"../../stable-diffusion-webui/tmp/SD1/aobp/ABPModel-ep59.safetensors\", \n",
    "        \"../../stable-diffusion-webui/models/Stable-diffusion/sample-nd-epoch59.safetensors\",\n",
    "        \"./json/test.json\",\n",
    "        \"./img/test.png\",\n",
    "        \"l2\",\n",
    "        True\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72faedc9",
   "metadata": {},
   "source": [
    "The compare loop. `tqdm` may not work, at least for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cfa038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_cmp(argv):\n",
    "    # Nasty unpacking but it works.\n",
    "    pab = argv[0] \n",
    "    ofp0 = argv[1]\n",
    "\n",
    "    pak = pab[0][0]\n",
    "    pav = pab[0][1]\n",
    "    pbk = pab[1][0]\n",
    "    pbv = pab[1][1]\n",
    "    ofjp = \"{}{}_{}_{}.json\".format(ofp_folder['json'], ofp0, pak, pbk)\n",
    "    ofip = \"{}{}_{}_{}.png\".format(ofp_folder['img'], ofp0, pak, pbk)\n",
    "\n",
    "    #print(\"{} vs {}\".format(pav, pbv))\n",
    "    cmp_json(pak, pbk, pav, pbv, ofjp, ofip, \"l2\", True)\n",
    "    return ofp0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a299e852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many files to generate: No intermediate files will be generated.\n",
      "26796 model pairs to be compared.\n",
      "Parallel mode. Progress bar may not show linear progress.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c906dfb3e9d461c8b490f41e942aff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "print(\"Too many files to generate: No intermediate files will be generated.\")\n",
    "cmp_count = len(cmp_mapping[0][1])\n",
    "ofp0 = cmp_mapping[0][0]\n",
    "print(\"{} model pairs to be compared.\".format(cmp_count))\n",
    "print(\"Parallel mode. Progress bar may not show linear progress.\")\n",
    "res_cmp = thread_map(exec_cmp, [(pab, ofp0) for pab in cmp_mapping[0][1]], max_workers=g_threads)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39e6b8bb",
   "metadata": {},
   "source": [
    "End of the comparasion loop. Dump result again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42268bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/26646362/numpy-array-is-not-json-serializable\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if torch.is_tensor(obj):\n",
    "            obj = obj.cpu().numpy()\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3f49b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmj[\"dm\"] = cmp_dm\n",
    "with open(model_map_o, \"w\") as fj:\n",
    "    json.dump(cmj, fj, indent=4, sort_keys=True, cls=NumpyEncoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e93469a",
   "metadata": {},
   "source": [
    "Now here is the \"part 2\": Visualize the distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08f17d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.dizzycoding.com/drawing-a-graph-or-a-network-from-a-distance-matrix/\n",
    "# https://thispointer.com/python-how-to-convert-a-list-to-dictionary/\n",
    "# https://stackoverflow.com/questions/46784028/edge-length-in-networkx\n",
    "# https://stackoverflow.com/questions/60397606/how-to-round-off-values-corresponding-to-edge-labels-in-a-networkx-graph\n",
    "# https://stackoverflow.com/questions/75078531/edge-length-based-on-distance/75083544#75083544\n",
    "# https://stackoverflow.com/questions/3567018/how-can-i-specify-an-exact-output-size-for-my-networkx-graph\n",
    "# https://stackoverflow.com/questions/26691442/how-do-i-add-a-new-attribute-to-an-edge-in-networkx\n",
    "# https://stackoverflow.com/questions/18911994/visualize-distance-matrix-as-a-graph\n",
    "\n",
    "def plot_vg(dmk, dmv, d):\n",
    "    \n",
    "    mgm = { i : model_list[i] for i in range(0, len(model_list) ) }\n",
    "    initialpos = { i : (i,i) for i in range(0, len(model_list) )}\n",
    "\n",
    "    dmv2 = dmv.cpu().numpy() if torch.is_tensor(dmv) else dmv\n",
    "\n",
    "    G = nx.from_numpy_array(dmv2)\n",
    "\n",
    "    #np.zero = no such comparasion = can skip.\n",
    "    if nx.number_of_edges(G) == 0:\n",
    "        return\n",
    "\n",
    "    G = nx.relabel_nodes(G, mgm)    \n",
    "    og = \"{}{}_{}_vg.png\".format(ofp_folder['img'], model_group_key, dmk)\n",
    "\n",
    "    # Argh the weight is inverse...\n",
    "    for eg in G.edges(data=True):\n",
    "        #print(eg)\n",
    "        #print(eg[2])\n",
    "        egv =  0.0 if eg[2]['weight'] == 0.0 else (1.0 / eg[2]['weight'])\n",
    "        #print(egv)\n",
    "        eg[2]['inverse_weight'] = egv\n",
    "\n",
    "    # Using the unnormalized Laplacian, the layout shows possible clusters of nodes which are an approximation of the ratio cut.\n",
    "    #pos = nx.spring_layout(G, weight='inverse_weight', pos=initialpos, seed=g_seed)\n",
    "    pos = nx.spectral_layout(G, weight='inverse_weight')\n",
    "    edge_labels = dict([((u,v,), f\"{d['weight']:.4f}\") for u,v,d in G.edges(data=True)])\n",
    "\n",
    "    plt.figure(1, figsize=(16,12)) \n",
    "\n",
    "    nx.draw_networkx(G, pos)\n",
    "    nx.draw_networkx_labels(G, pos)\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "   \n",
    "    plt.title(\"{} Distance of {} in model group {}\".format(d.upper(), dmk.upper(), model_group_key))\n",
    "    plt.savefig(og)\n",
    "    #plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d079df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cdmk in cmp_dm.keys():\n",
    "    plot_vg(cdmk, cmp_dm[cdmk], \"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf14b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotline(dmk, dmv, d):\n",
    "    #Assume models are arranged in sequence\n",
    "    x_arr = [m[:10] for m in model_list]\n",
    "    y_arr = dmv[0] \n",
    "\n",
    "    og = \"{}{}_{}_xy.png\".format(ofp_folder['img'], model_group_key, dmk)\n",
    "\n",
    "    plt.figure(1, figsize=(16,12)) \n",
    "\n",
    "    plt.plot(x_arr, y_arr, label='line')\n",
    "\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(d.upper())\n",
    "\n",
    "    plt.title(\"{} Distance of {} in model group {}\".format(d.upper(), dmk.upper(), model_group_key))\n",
    "    plt.savefig(og)\n",
    "    #plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a0546fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cdmk in cmp_dm.keys():\n",
    "    plotline(cdmk, cmp_dm[cdmk], \"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cf1db38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare: 26796, time: 110933 sec\n"
     ]
    }
   ],
   "source": [
    "te = time.time()\n",
    "print(\"Compare: {}, time: {} sec\".format(cmp_count, int(te - ts)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "503b9db4-4a84-4988-a51e-31493502beeb",
   "metadata": {},
   "source": [
    "### Findings (VAE) ###\n",
    "- `kl-f8` vs SD prune: Some layers are pruned.\n",
    "- `kl-f8` vs WD1: Both `encoder`, `decoder` is trained\n",
    "- WD1 vs WD2: Only `decoder` is trained\n",
    "- `kl-f8` vs NAI: Only `decoder` is trained. **However it is same as SD v1.4 bundled. See below.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "791c2176",
   "metadata": {},
   "source": [
    "### Findings (NAI) ###\n",
    "- SD 7G vs SD 4G: EMA pruned. *Applies for all models*.\n",
    "- SD 7G vs NAI 7G: **Same \"text encoder\" (renamed layer) and \"VAE\".**\n",
    "- ACertainty: \"Seriously fine-tuned from SD with tons of (NAI) AIGC.\" **confirmed.** However VAE Decoder is different."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8644af64-fc44-4c77-9a2b-631f7c8ead0b",
   "metadata": {},
   "source": [
    "### Findings (SD Variant) ###\n",
    "- momoko-e: **Dreambooth**. Text encoder is **partially changed.**\n",
    "- Anything v3 / BasilMix / Anything v4 etc.: **Merged model. All layers are changed.**\n",
    "- NAI: Some `cumprod` layers dropped\n",
    "- ANY3: Same as NAI (merged)\n",
    "- AC: Same as NAI (???)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a441a75-512b-495c-8263-d448a3580108",
   "metadata": {},
   "source": [
    "### Findings (NMFSAN) ###\n",
    "- NMFSAN: No `cond_stage_model` = Load \"last text encoder\" or `None` (will generate glitched images). No `first_stage_model` = Must load VAE (`same_model_name.vae.pt`).\n",
    "- Currently called \"negative textual inversion\". Freeze TE train UNET > Make TI > Freeze TE train UNET again. i.e. No TE no VAE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "193a86a4",
   "metadata": {},
   "source": [
    "### Findings (BPModel) ###\n",
    "- I have internal versions started from \"Stupid Diffusion\", another internal version of ACertainty.\n",
    "- This is not a strict and formal proof, but I expect the L2 distances will align with a **almost flat plane**, to show some meaning for such comparasion.\n",
    "- This **must not the exact same plane** because the BPModel was trained with changed dataset and configuration (for example, ARB setting / adding subset of datasets / \"negative-TI\" trick). However the iterlation should show a somewhat \"clear way of improvement\".\n",
    "\n",
    "|Model A|Model B|others|attn1|attn2|\n",
    "|---|---|---|---|---|\n",
    "|AC|mk0|96.0301|23.2504|26.0299|\n",
    "|mk0|mk3|51.6394|16.5754|13.6055|\n",
    "|mk3|mk5|20.2971|8.6033|4.6885|\n",
    "|mk5|nman|22.4227|7.5967|5.7737|\n",
    "|L1(A)|L1(B)|190.3893|56.0258|50.0976|\n",
    "|L2(A)|L2(B)|**113.1510**|**30.7742**|**30.2982**|\n",
    "|AC|nman|**113.7759**|**35.8125**|**30.5798**|\n",
    "\n",
    "- Somehow L2 can reflect \"direction between the models\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71cba828",
   "metadata": {},
   "source": [
    "### Findings (SD 2.x) ###\n",
    "- WD v1.4: Text encoder and VAE encoder is changed.\n",
    "- CJD v2.1.1: VAE encoder is changed.\n",
    "- J's RD: Text encoder and VAE are **uncahnged**.\n",
    "- P1at's merge: VAE is unchanged, but everything else is changed.\n",
    "- SD 1.x vs 2.x: Text encoder is entirely swithced. Some layers' dimension is changed. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af6658c8",
   "metadata": {},
   "source": [
    "## Discussion ##\n",
    "- \"Ignoring prompts\" (where is `astolfo`? No human!) is caused by **bias in text encoder?**\n",
    "- \"Missing details\" (given some element of the entity is present, e.g. `astolfo` has `pink_hair` but `1girl`) is caused by **bias in UNET?**\n",
    "- That's why Anything v3 (20 / 20 momoco style with minimal negative prompts) is popular because the task (waifu AIGC) is a narrow objective which favours bias?\n",
    "- BPModel is commented \"hard to use\" becasue it relies on original SD text encoder and VAE? However diversity is maximum with most art style is succesfully trained? Original SD has such \"artist prompts\", e.g. Vincent Van Gogh.\n",
    "- Why ACertainty looks like NAI? AIGC dataset as informal Reinforcement Learning?\n",
    "- SD 2.x is so broken becasuse the CLIP? Or the UNET? Why WD 1.4 E1 ignores prompts (where's `astolfo`? No `1boy`!) but start listening prompts in E2 (must include `quality:0` but `pink_hair`, `1boy` is OK)?\n",
    "- J's RD fails just because the original SD 2.x text encoder is so bad? Applies for CJD also (where's `astolfo`? No `1boy`!)?\n",
    "- Why BasilMix works (nice merge with chosen hyperparameters)?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4206b759",
   "metadata": {},
   "source": [
    "# Further work #\n",
    "- Compare for a set of models **with a clear relatiion**. For example, [merging ratio](https://huggingface.co/ThePioneer/CoolerWaifuDiffusion) and [training epoches](https://huggingface.co/AnnihilationOperator/ABPModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35610b14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "novelai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
