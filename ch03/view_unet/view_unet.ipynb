{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising UNET #\n",
    "\n",
    "### Abstract ###\n",
    "\n",
    "- Self explained. Using `torch_view` as main library.\n",
    "- Some notations are borrowed from [mega_cmp.ipynb](./v2a/mmega_cmp.ipynb)\n",
    "- **Warning:** [Make sure your C Drive is huge.](https://huggingface.co/docs/datasets/cache) Default directory is `C:\\Users\\User\\.cache\\huggingface\\hub`\n",
    "- Also make sure token is present in `C:\\Users\\User\\.cache\\huggingface\\token`.\n",
    "\n",
    "### Required libraries ###\n",
    "\n",
    "- ~~Should be the common ML pack we're using. Also with [SD webui's dependency](https://github.com/AUTOMATIC1111/stable-diffusion-webui).~~\n",
    "\n",
    "- [torchview](https://torchview.dev/)\n",
    "- [safetensors](https://huggingface.co/docs/safetensors/index)\n",
    "- [diffusers](https://huggingface.co/docs/diffusers/installation)\n",
    "- [omegaconf](https://anaconda.org/conda-forge/omegaconf)\n",
    "- [pytorch](https://pytorch.org/get-started/locally/#windows-python)\n",
    "- [Graphviz](https://graphviz.org/)\n",
    "- [torchinfo](https://pypi.org/project/torchinfo/)\n",
    "\n",
    "### Some layer name to interprept ###\n",
    "\n",
    "- Whole model combined as called `DiffusionPipeline` in [Diffusers](https://huggingface.co/docs/diffusers/index).\n",
    "\n",
    "|Layer name|Description|Class name in Diffusers|\n",
    "|---|---|---|\n",
    "|`first_stage_model`|VAE|`AutoencoderKL`|\n",
    "|`cond_stage_model`|Text Encoder (SD1, SD2)|`CLIPTextModel`|\n",
    "|`conditioner.embedders.0`|Text Encoder 1 (SDXL)|`CLIPTextModel`|\n",
    "|`conditioner.embedders.1`|Text Encoder 2 (SDXL)|`CLIPTextModelWithProjection`|\n",
    "|`model.diffusion_model`|UNET|`UNet2DConditionModel`|\n",
    "|`model_ema`|EMA model for training|n/a|\n",
    "|`cumprod`, `betas`, `alphas`|`CosineAnnealingLR`|n/a|\n",
    "\n",
    "### Some notation (Useful in the bin chart) ###\n",
    "- `attn1`: `sattn` = *Self attention*\n",
    "- `attn2`: `xattn` = *Cross attention*\n",
    "- `ff`: *Feed forward*\n",
    "- `norm`: [Normalisation layer](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html). `elementwise_affine=True` introduces trainable `bias` and `weight`. \n",
    "- `proj`: *Projection*\n",
    "- `emb_layers`: *Embedding layers*\n",
    "- `mlp`: *Multilayer perceptron*\n",
    "- `others`: `ff` + `norm` + `proj` + `emb_layers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'svg'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import diffusers\n",
    "import accelerate\n",
    "import transformers\n",
    "import huggingface_hub\n",
    "from safetensors.torch import load_file #safe_open\n",
    "from diffusers import UNet2DConditionModel, SD3Transformer2DModel, FluxTransformer2DModel\n",
    "\n",
    "from torchview import draw_graph\n",
    "from torchinfo import summary\n",
    "\n",
    "import graphviz\n",
    "graphviz.set_jupyter_format('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu124\n",
      "0.30.0\n",
      "4.44.0\n",
      "0.33.0\n",
      "0.24.5\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(diffusers.__version__)\n",
    "print(transformers.__version__)\n",
    "print(accelerate.__version__)\n",
    "print(huggingface_hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for OMP: Error #15\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU for Flux.\n",
    "g_device = \"cpu\" #\"cuda:0\" \n",
    "# Currently for generating graph only.\n",
    "g_seed = 114514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model path\n",
    "model_path = {\n",
    "    \"sd1\": \"runwayml/stable-diffusion-v1-5\",\n",
    "    \"sd2\": \"stabilityai/stable-diffusion-2-1\",\n",
    "    \"sdxl\": \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    \"sd3\": \"stabilityai/stable-diffusion-3-medium-diffusers\",\n",
    "    \"flux\": \"black-forest-labs/FLUX.1-dev\",\n",
    "}\n",
    "\n",
    "model_type = torch.float16 if \"cuda\" in g_device else torch.float # CPU doesn't support FP16 / FP8\n",
    "long_type = torch.int64 if \"cuda\" in g_device else torch.long # CPU doesn't support FP16 / FP8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only online model is available.\n",
    "\n",
    "`load_single_file` is failed (versioning hell, omitted). I load online model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_instance = None # Clear\n",
    "unet_instance = {} # Clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We run it later\n",
    "# for k in model_path.keys():\n",
    "#    unet_instance[k] = UNet2DConditionModel.from_pretrained(model_path[k], subfolder=\"unet\",  torch_dtype=torch.float16).to(g_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input size is trial and error.\n",
    "\n",
    "Not actually, we can read `config.json` form the actual official model in HuggingFace, and `nn.Module` has already created with the config. \n",
    "\n",
    "Originally it is scattered in different Git Repos, but HF does a great job here.\n",
    "\n",
    "- `sd1`: [stabilityai/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/unet/config.json)\n",
    "- `sd2`: [stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/unet/config.json)\n",
    "- `sdxl`: [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/unet/config.json)\n",
    "- `sd3`: [stabilityai/stable-diffusion-3-medium-diffusers](https://huggingface.co/stabilityai/stable-diffusion-3-medium-diffusers/blob/main/transformer/config.json)\n",
    "- `flux`: [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/transformer/config.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not Used.\n",
    "input_data_mapping_sample = {\n",
    "    \"sd1\": {\n",
    "        'sample': torch.rand(1, 4, 64, 64).type(model_type).to(g_device),\n",
    "        'timestep': torch.rand(1).type(model_type).to(g_device),\n",
    "        'encoder_hidden_states': torch.rand(1, 77, 768).type(model_type).to(g_device),\n",
    "    },\n",
    "    \"sd2\": {\n",
    "        'sample': torch.rand(1, 4, 96, 96).type(model_type).to(g_device),\n",
    "        'timestep': torch.rand(1).type(model_type).to(g_device),\n",
    "        'encoder_hidden_states': torch.rand(1280, 77, 1024).type(model_type).to(g_device),\n",
    "    },\n",
    "    \"sdxl\": {\n",
    "        'sample': torch.rand(1, 4, 128, 128).type(model_type).to(g_device),\n",
    "        'timestep': torch.rand(1).type(model_type).to(g_device),\n",
    "        'encoder_hidden_states': torch.rand(1280, 77, 2048).type(model_type).to(g_device),\n",
    "        'added_cond_kwargs': {\n",
    "            'text_embeds': torch.rand(1280, 2560).type(model_type).to(g_device),\n",
    "            'time_ids': torch.rand(1280).type(model_type).to(g_device),\n",
    "        },\n",
    "    },\n",
    "    \"sd3\": {\n",
    "        'hidden_states': torch.rand(1, 16, 128, 128).type(model_type).to(g_device),\n",
    "        'timestep': torch.ones((1536, )).type(long_type).to(g_device),\n",
    "        'encoder_hidden_states': torch.rand(1, 77, 4096).type(model_type).to(g_device),\n",
    "        'pooled_projections': torch.rand(1, 2048).type(model_type).to(g_device)\n",
    "    },\n",
    "    \"flux\": {\n",
    "        'hidden_states': torch.rand(1, 4096, 64).type(model_type).to(g_device),\n",
    "        'timestep': torch.ones((1, )).type(long_type).to(g_device),\n",
    "        'guidance': torch.zeros((1, )).type(long_type).to(g_device),\n",
    "        'encoder_hidden_states': torch.rand(1, 256, 4096).type(model_type).to(g_device),\n",
    "        'pooled_projections': torch.rand(1, 768).type(model_type).to(g_device),\n",
    "        'txt_ids': torch.rand(1, 256, 3).type(model_type).to(g_device),\n",
    "        'img_ids': torch.rand(1, 4096, 3).type(model_type).to(g_device)\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Graph output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_paths = {\n",
    "    #\"sd1\": \"./sd1_unet\",\n",
    "    #\"sd2\": \"./sd2_unet\",\n",
    "    #\"sdxl\": \"./sdxl_unet\", \n",
    "    #\"sd3\": \"./sd3_mmdit\",\n",
    "    \"flux\": \"./flux_mmdit\", # 31 minutes on CPU\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For in place generation (requested)\n",
    "png_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main loop. Note that `input_data` is generated inplace. The expected dimension is already available in `model.config` mentioned above.\n",
    "\n",
    "Notice that we need both `depth=1` and `depth=2` to **link MBW layers**.\n",
    "\n",
    "Also it keep the diagram elegent (however the count won't match!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercepted from FluxPipeline\n",
    "# C:\\ProgramData\\Miniconda3\\envs\\novelai-env\\Lib\\site-packages\\diffusers\\pipelines\\flux\n",
    "if False:\n",
    "    #torch.Size([1, 4096, 64])\n",
    "    print(hidden_states.shape)\n",
    "    #tensor([1000.0000,  904.5308,  759.5109,  512.8441], device='cuda:0') / 1000\n",
    "    print(timestep.shape)\n",
    "    #tensor([0.], device='cuda:0')\n",
    "    print(guidance.shape)\n",
    "    #torch.Size([1, 768])\n",
    "    print(pooled_projections.shape)\n",
    "    #torch.Size([1, 256, 4096])\n",
    "    print(encoder_hidden_states.shape)\n",
    "    #torch.Size([1, 256, 3])\n",
    "    print(txt_ids.shape)\n",
    "    #torch.Size([1, 4096, 3])\n",
    "    print(img_ids.shape)\n",
    "    #None\n",
    "    print(joint_attention_kwargs)                \n",
    "    raise Exception(\"We are here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mbw_component(cur_unet):\n",
    "    if cur_unet == \"sd1\":\n",
    "        return UNet2DConditionModel.from_pretrained(model_path[cur_unet], subfolder=\"unet\", torch_dtype=model_type).to(g_device)\n",
    "    elif cur_unet == \"sd2\":\n",
    "        return UNet2DConditionModel.from_pretrained(model_path[cur_unet], subfolder=\"unet\", torch_dtype=model_type).to(g_device)\n",
    "    elif cur_unet == \"sdxl\":\n",
    "        return UNet2DConditionModel.from_pretrained(model_path[cur_unet], subfolder=\"unet\", torch_dtype=model_type).to(g_device)\n",
    "    elif cur_unet == \"sd3\":\n",
    "        return SD3Transformer2DModel.from_pretrained(model_path[cur_unet], subfolder=\"transformer\", torch_dtype=model_type).to(g_device)\n",
    "    elif cur_unet == \"flux\":\n",
    "        return FluxTransformer2DModel.from_pretrained(model_path[cur_unet], subfolder=\"transformer\", guidance_embeds=True, torch_dtype=model_type).to(g_device) #\n",
    "\n",
    "def get_feature_dim(cur_unet):\n",
    "    # Should fit CLIPTextModel.hidden_size, 2048 = 768 + 1280 for SDXL\n",
    "    if cur_unet == \"sd1\":\n",
    "        return unet_instance[cur_unet].config.cross_attention_dim\n",
    "    elif cur_unet == \"sd2\":\n",
    "        return unet_instance[cur_unet].config.cross_attention_dim\n",
    "    elif cur_unet == \"sdxl\":\n",
    "        return unet_instance[cur_unet].config.cross_attention_dim\n",
    "    elif cur_unet == \"sd3\":\n",
    "        return unet_instance[cur_unet].config.joint_attention_dim\n",
    "    elif cur_unet == \"flux\":\n",
    "        return unet_instance[cur_unet].config.joint_attention_dim\n",
    "\n",
    "def get_sample_height(cur_unet):\n",
    "    # IDK why Flux doesn't include the \"128\" as sample_size\n",
    "    if cur_unet == \"sd1\":\n",
    "        return unet_instance[cur_unet].config.sample_size\n",
    "    elif cur_unet == \"sd2\":\n",
    "        return unet_instance[cur_unet].config.sample_size\n",
    "    elif cur_unet == \"sdxl\":\n",
    "        return unet_instance[cur_unet].config.sample_size\n",
    "    elif cur_unet == \"sd3\":\n",
    "        return unet_instance[cur_unet].config.sample_size\n",
    "    elif cur_unet == \"flux\":\n",
    "        # Will drop eventually.\n",
    "        return 64 #default_sample_size\n",
    "    \n",
    "def get_sequence_length(cur_unet):\n",
    "    # IDK why Flux doesn't include the \"128\" as sample_size\n",
    "    if cur_unet == \"sd1\":\n",
    "        return 77 # See CLIPTextModel.max_position_embeddings\n",
    "    elif cur_unet == \"sd2\":\n",
    "        return 77 # See CLIPTextModel.max_position_embeddings\n",
    "    elif cur_unet == \"sdxl\":\n",
    "        return 77 # See CLIPTextModel.max_position_embeddings\n",
    "    elif cur_unet == \"sd3\":\n",
    "        return 77 # See CLIPTextModel.max_position_embeddings\n",
    "    elif cur_unet == \"flux\":\n",
    "        return 256 #base_seq_len\n",
    "\n",
    "def main_loop(cur_unet):\n",
    "    #240807: It is no longer \"UNet\" but we can still treat it as \"that particular part of diffusion model\"\n",
    "    unet_instance[cur_unet] = get_mbw_component(cur_unet) if cur_unet not in unet_instance else unet_instance[cur_unet]\n",
    "\n",
    "    sequence_length = get_sequence_length(cur_unet)\n",
    "    feature_dim = get_feature_dim(cur_unet)\n",
    "    height = get_sample_height(cur_unet)\n",
    "    width = height #square is fine\n",
    "    channel = unet_instance[cur_unet].config.in_channels\n",
    "    step = 1 #arbitary single float\n",
    "    batch = 1 #1bs\n",
    "\n",
    "    inplace_input_data = {\n",
    "        'sample': torch.rand(batch, channel, height, width).type(model_type).to(g_device),\n",
    "        'timestep': torch.rand(step).type(model_type).to(g_device),\n",
    "        'encoder_hidden_states': torch.rand(batch, sequence_length, feature_dim).type(model_type).to(g_device),\n",
    "    }\n",
    "\n",
    "    # SDXL special\n",
    "    if cur_unet == \"sdxl\":\n",
    "        addition_time_embed_dim = unet_instance[cur_unet].config.addition_time_embed_dim\n",
    "        projection_class_embeddings_input_dim = unet_instance[cur_unet].config.projection_class_embeddings_input_dim\n",
    "        time_sequence_length = projection_class_embeddings_input_dim - addition_time_embed_dim\n",
    "        inplace_input_data['added_cond_kwargs'] = {\n",
    "            'text_embeds': torch.rand(batch, time_sequence_length).type(model_type).to(g_device),\n",
    "            'time_ids': torch.rand(batch).type(model_type).to(g_device),\n",
    "        }\n",
    "\n",
    "    # SD3 special\n",
    "    if (cur_unet == \"sd3\") or (cur_unet == \"flux\"):\n",
    "        projection_dim = unet_instance[cur_unet].config.pooled_projection_dim\n",
    "\n",
    "        # Drop sample\n",
    "        del inplace_input_data['sample']\n",
    "        inplace_input_data['hidden_states'] = torch.rand(batch, channel, height, width).type(model_type).to(g_device)      \n",
    "        inplace_input_data['pooled_projections'] = torch.rand(batch, projection_dim).type(model_type).to(g_device)\n",
    "        inplace_input_data['timestep'] = torch.ones((batch, )).type(long_type).to(g_device) #torch.ones((inner_dim, ), dtype=torch.long).to(g_device)\n",
    "\n",
    "        if (cur_unet == \"flux\"):\n",
    "            #height * width (64*64) instead of feature_dim (4096)\n",
    "            inplace_input_data['hidden_states'] = torch.rand(batch, height * width, channel).type(model_type).to(g_device)\n",
    "            inplace_input_data['guidance'] = torch.zeros((batch, )).type(model_type).to(g_device)\n",
    "            inplace_input_data['txt_ids'] = torch.zeros(batch, sequence_length, 3).type(model_type).to(g_device)\n",
    "            inplace_input_data['img_ids'] = torch.zeros(batch, height * width, 3).type(model_type).to(g_device)\n",
    "        \n",
    "    model_summary = summary(unet_instance[cur_unet], \n",
    "        input_data=inplace_input_data, \n",
    "        col_names=(\"input_size\", \"output_size\", \"num_params\")\n",
    "    )\n",
    "\n",
    "    with open(filename_paths[cur_unet] + '.txt', 'w') as the_file:\n",
    "        the_file.write(str(model_summary))\n",
    "\n",
    "    unet_png = draw_graph(unet_instance[cur_unet], \n",
    "        input_data=inplace_input_data, \n",
    "        graph_name=model_path[cur_unet], \n",
    "        device=g_device, mode=\"eval\", \n",
    "        depth=1,     \n",
    "        \n",
    "        roll=True,        \n",
    "        save_graph=True,\n",
    "        filename=filename_paths[cur_unet]\n",
    "    ) #expand_nested=True, hide_inner_tensors=False,   \n",
    "    png_results[cur_unet] = unet_png\n",
    "\n",
    "    unet_png_2 = draw_graph(unet_instance[cur_unet], \n",
    "        input_data=inplace_input_data, \n",
    "        graph_name=model_path[cur_unet], \n",
    "        device=g_device, mode=\"eval\", \n",
    "        depth=2,       \n",
    "        expand_nested=True,\n",
    "        roll=True,        \n",
    "        save_graph=True,\n",
    "        filename='{}_2'.format(filename_paths[cur_unet])\n",
    "    ) #hide_inner_tensors=False, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `sd1`: Should run with no problem\n",
    "- `sd2`: May OOM, however I'm using RTX 3090 now.\n",
    "- `sdxl`: This is tricky: No docuement. [Read code](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py) for workaround.\n",
    "- `sd3`: Login required. Paste the token in `C:\\Users\\User\\.cache\\huggingface\\token`.\n",
    "- `flux`: Same as `sd3`. Also it may be huge AF because I'm using the dev version. CPU mode only! Also requires newest HF libraries! No Doc, only codes in [HF](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformers/transformer_flux.py) and [OG repo](https://github.com/black-forest-labs/flux/blob/main/src/flux/model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618eda860ad94203be6cdc5fa77252d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(process:38172): Pango-WARNING **: 11:39:31.124: couldn't load font \"Linux libertine Not-Rotated 10\", falling back to \"Sans Not-Rotated 10\", expect ugly output.\n",
      "\n",
      "(process:16536): Pango-WARNING **: 11:50:05.310: couldn't load font \"Linux libertine Not-Rotated 10\", falling back to \"Sans Not-Rotated 10\", expect ugly output.\n",
      "dot: graph is too large for cairo-renderer bitmaps. Scaling by 0.755749 to fit\n"
     ]
    }
   ],
   "source": [
    "for k in filename_paths:\n",
    "    main_loop(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate the image inplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#png_results[\"sd1\"].visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#png_results[\"sd2\"].visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#png_results[\"sdxl\"].visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may extend the work below later. Now we block the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: ERROR"
     ]
    }
   ],
   "source": [
    "raise Exception(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all diagrams and summaries are generated. Now we can try to map MBW layers (`IN00-11`, `MID`, `OUT00-11`) to the diagram.\n",
    "\n",
    "Note that local models are used instead. Also only `safetensors` are used.\n",
    "\n",
    "This time models is read as files, and I'm almost OOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_path = {\n",
    "    \"sd1\": \"../../stable-diffusion-webui/tmp/view_unet/21b-AstolfoMix-2020b.safetensors\",\n",
    "    \"sd2\": \"../../stable-diffusion-webui/tmp/view_unet/wd-1-5-beta2-fp16.safetensors\",\n",
    "    \"sdxl\": \"../../stable-diffusion-webui/tmp/view_unet/wdxl-aesthetic-0.9.safetensors\",\n",
    "}\n",
    "\n",
    "local_models = {}\n",
    "\n",
    "for k in local_model_path.keys():\n",
    "    local_models[k] = load_file(local_model_path[k], device='cpu')\n",
    "\n",
    "# I don't even have time to have sneek peek on SDXL models... Let's crack them here\n",
    "max_layers = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_file` only return a HUGE `dict`. You can use JSON library to visualize it, but it is not useful. There is no linkage between the layers.\n",
    "\n",
    "First, make a nice name finder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_a_keyword_in_a_layer(layer, *keywords):\n",
    "    for k in keywords:\n",
    "         if k not in layer:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def do_some_keywords_in_some_layers(model, *keywords):\n",
    "    for layer in list(model.keys()):\n",
    "        if do_a_keyword_in_a_layer(layer, *keywords):\n",
    "            return layer\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read the summary and the png, there are some noticeable layers: \n",
    "\n",
    "|Layer Name|IN / MID / OUT?|Presence of Identifible Layers?|Layer in `sd1` and `sd2`|Layer in SDXL|\n",
    "|---|---|---|---|---|\n",
    "|`Conv2d`|`input_blocks`|n/a|`IN00`|`IN00`|\n",
    "|`CrossAttnDownBlock2D` > `Transformer2DModel`|`input_blocks`|`transformer_blocks`|`IN01`,`IN02`,`IN04`,`IN05`,`IN07`,`IN08`|`IN04`,`IN05`,`IN07`,`IN08`|\n",
    "|`CrossAttnDownBlock2D` > `DownSample2D`|`input_blocks`|`op`|`IN03`,`IN06`,`IN09`|`IN03`,`IN06`|\n",
    "|`DownBlock2D` > `ResnetBlock2D`|`input_blocks`|`in_layers`|`IN10`,`IN11`|`IN01`,`IN02`|\n",
    "|`UNetMidBlock2DCrossAttn`|`middle_block`|`transformer_blocks`|`MID`|`MID`|\n",
    "|`UpBlock2D` > `ResnetBlock2D`|`output_blocks`|`in_layers`|`OUT00`,`OUT01`,`OUT02`|`OUT06`,`OUT07`,`OUT08`|\n",
    "|`CrossAttnUpBlock2D` > `Transformer2DModel`|`output_blocks`|`transformer_blocks`|`OUT03`,`OUT04`,`OUT05`,`OUT06`,`OUT07`,`OUT08`,`OUT09`,`OUT10`,`OUT11`|`OUT00`,`OUT01`,`OUT02`,`OUT03`,`OUT04`,`OUT05`|\n",
    "|`UpBlock2D` > `UpSample2D`|`output_blocks`|`conv`|`OUT02`|n/a|\n",
    "|`CrossAttnUpBlock2D` > `UpSample2D`|`output_blocks`|`conv`|`OUT05`,`OUT08`|`OUT02`,`OUT05`|\n",
    "|`GroupNorm`, `Conv2d`|`output_blocks`|n/a|`OUT`|`OUT`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streadgy ###\n",
    "\n",
    "Note that `block_types` are tested insequence. Larger layers will be indentified first (usually spotted in layer1), then the smaller layers in layer 2, note that most of them has some indentifiers (not unique) to be indentified.\n",
    "\n",
    "### Findings ###\n",
    "\n",
    "IN00, IN03, IN06, IN09 is suprisingly low in layers count.\n",
    "\n",
    "**The Tensor size may not match the summary / png.** It is becase my 64x64 \"image size\" may be compressed into just 3x3 in actual tensor block as in latent space. Also, my \"batch size\" is \"1\" for single image. However it may be \"320 tokens\" after transforming in CLIP. **However 2nd parameter remains constent as shown in most images in the internet.**\n",
    "\n",
    "For most neuron layers, it is $Y=SiLU(WX+B)$ ([SiLU activation](https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html) for [common terms](https://medium.com/@okanyenigun/building-a-neural-network-from-scratch-in-python-a-step-by-step-guide-8f8cab064c8a)), which `weight` is a tensor, and `bias` is a scalar only. We look for tesnsor size.\n",
    "\n",
    "|Model|Size in actual tensor|Size in summary|\n",
    "|---|---|---|\n",
    "|`sd1`|`[320, 4, 3, 3]`|`[1, 4, 64, 64]`|\n",
    "|`sd1`|`[320, 320, 3, 3]`|`[1, 320, 32, 32]`|\n",
    "|`sd1`|`[640, 640, 3, 3]`|`[1, 640, 16, 16]`|\n",
    "|`sd1`|`[1280, 1280, 3, 3]`|`[1, 1280, 8, 8]`|\n",
    "\n",
    "`IN00` doesn't have `op` marked. Also it has `4` channels, therefore it could be `Conv2D`.\n",
    "\n",
    "For `CrossAttnDownBlock2D` in layer1, `DownSample2D` comes after 2x `Transformer2DModel`, resembles `IN03` comes after `IN01` and `IN02` and before `IN04`, which is plausible.\n",
    "\n",
    "Same pattern applies to `DownBlock2D`, then `IN10` and `IN11` are identified.\n",
    "\n",
    "However `UpBlock2D` is a bit different: `UpSample2D` doesn't follow `DownSample2D`, it has `conv` instead of `op`, and the id is different. \n",
    "\n",
    "`OUT11` doesn't have `UpSample2D` even it is `CrossAttnUpBlock2D` in layer 1. *What an asymmetry*.\n",
    "\n",
    "`OUT` has no suffix like `IN00` also. However `OUT` has *2 distinct layers* with same size: `GroupNorm` and  `Conv2d`.\n",
    "\n",
    "SD2 is almost identical with SD1, with only slight size in difference.\n",
    "\n",
    "SDXL is **different** with SD1. The greatest difference is `UpBlock2D` is now **after** `CrossAttnUpBlock2D` instead of before them. Also `UpBlock2D` doesn't have `UpSample2D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_interested_layers(model, layers_count = max_layers):\n",
    "    block_layers = ['input_blocks.0','input_blocks','middle_block','output_blocks','.out']\n",
    "    block_types = ['transformer_blocks','in_layers','conv','op']\n",
    "\n",
    "    for b_l in block_layers:\n",
    "        for i in range(layers_count):\n",
    "            # Search for direct layers (not much)\n",
    "            direct_layer = do_some_keywords_in_some_layers(model, \"{}.{}.{}\".format(b_l, i, 'weight'))\n",
    "            if direct_layer:\n",
    "                print('{}: {}'.format(direct_layer, model[direct_layer].size()))\n",
    "            else:\n",
    "                for b_t in block_types:\n",
    "                    serarch_result = do_some_keywords_in_some_layers(model, \"{}.{}.\".format(b_l, i), b_t)\n",
    "                    if serarch_result:\n",
    "                        print('{}: {}'.format(serarch_result, model[serarch_result].size()))\n",
    "                        break\n",
    "                # Special case: UpSample2D are embedded in some output_blocks\n",
    "                if (b_l == 'output_blocks'):\n",
    "                    is_upsample2d = do_some_keywords_in_some_layers(model, \"{}.{}.\".format(b_l, i), 'conv')\n",
    "                    if is_upsample2d:\n",
    "                        print('{}: {}'.format(is_upsample2d, model[is_upsample2d].size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.diffusion_model.input_blocks.0.0.weight: torch.Size([320, 4, 3, 3])\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.input_blocks.3.0.op.bias: torch.Size([320])\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.input_blocks.6.0.op.bias: torch.Size([640])\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.input_blocks.9.0.op.bias: torch.Size([1280])\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.middle_block.0.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.middle_block.2.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.0.bias: torch.Size([2560])\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.0.bias: torch.Size([2560])\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.0.bias: torch.Size([2560])\n",
      "model.diffusion_model.output_blocks.2.1.conv.bias: torch.Size([1280])\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.output_blocks.5.2.conv.bias: torch.Size([1280])\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.output_blocks.8.2.conv.bias: torch.Size([640])\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.out.0.weight: torch.Size([320])\n",
      "model.diffusion_model.out.2.weight: torch.Size([4, 320, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "list_interested_layers(local_models['sd1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.diffusion_model.input_blocks.0.0.weight: torch.Size([320, 4, 3, 3])\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.input_blocks.3.0.op.bias: torch.Size([320])\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.input_blocks.6.0.op.bias: torch.Size([640])\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.input_blocks.9.0.op.bias: torch.Size([1280])\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.middle_block.0.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.middle_block.2.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.0.bias: torch.Size([2560])\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.0.bias: torch.Size([2560])\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.0.bias: torch.Size([2560])\n",
      "model.diffusion_model.output_blocks.2.1.conv.bias: torch.Size([1280])\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.output_blocks.5.2.conv.bias: torch.Size([1280])\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.output_blocks.8.2.conv.bias: torch.Size([640])\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.out.0.weight: torch.Size([320])\n",
      "model.diffusion_model.out.2.weight: torch.Size([4, 320, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "list_interested_layers(local_models['sd2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.diffusion_model.input_blocks.0.0.weight: torch.Size([320, 4, 3, 3])\n",
      "model.diffusion_model.input_blocks.1.0.in_layers.0.bias: torch.Size([320])\n",
      "model.diffusion_model.input_blocks.2.0.in_layers.0.bias: torch.Size([320])\n",
      "model.diffusion_model.input_blocks.3.0.op.bias: torch.Size([320])\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.input_blocks.6.0.op.bias: torch.Size([640])\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.middle_block.0.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.middle_block.2.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.output_blocks.0.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.output_blocks.1.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.output_blocks.2.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.output_blocks.2.2.conv.bias: torch.Size([1280])\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.output_blocks.5.2.conv.bias: torch.Size([640])\n",
      "model.diffusion_model.output_blocks.6.0.in_layers.0.bias: torch.Size([960])\n",
      "model.diffusion_model.output_blocks.7.0.in_layers.0.bias: torch.Size([640])\n",
      "model.diffusion_model.output_blocks.8.0.in_layers.0.bias: torch.Size([640])\n",
      "model.diffusion_model.out.0.weight: torch.Size([320])\n",
      "model.diffusion_model.out.2.weight: torch.Size([4, 320, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "list_interested_layers(local_models['sdxl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
