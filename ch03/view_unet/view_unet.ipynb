{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising UNET #\n",
    "\n",
    "### Abstract ###\n",
    "\n",
    "- Self explained. Using `torch_view` as main library.\n",
    "- Some notations are borrowed from [mega_cmp.ipynb](./v2a/mmega_cmp.ipynb)\n",
    "- **Warning:** [Make sure your C Drive is huge.](https://huggingface.co/docs/datasets/cache) Default directory is `C:\\Users\\User\\.cache\\huggingface\\hub`\n",
    "- Also make sure token is present in `C:\\Users\\User\\.cache\\huggingface\\token`.\n",
    "- From the inconsistint result of `[\"sdxl\", \"sd1\", \"sd2\"]` **which was overestimated for 2.37x** (others are < 0.1%), I also implemented `diffusers` and `torch` native approach based from [this Stackoverflow post](https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model). Issues [#262](https://github.com/TylerYep/torchinfo/issues/262), [#303](https://github.com/TylerYep/torchinfo/issues/303), [#312](https://github.com/TylerYep/torchinfo/issues/312) were reported in `torchinfo`, which made me a bit panic. *Hopefully it can be justified from future inconsistent results.*\n",
    "- Refer [diffusers.num_parameters](https://huggingface.co/docs/diffusers/api/models/overview#diffusers.ModelMixin.num_parameters) [and its code](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/modeling_utils.py#L1040), [nn.Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html), [torch.numel](https://pytorch.org/docs/stable/generated/torch.numel.html) for how it is counted. It is very likely **MISMATCH** for other contents (e.g. `torchvision` and `torchinfo` here, refered as [\"model summary\"](https://stackoverflow.com/questions/42480111/how-do-i-print-the-model-summary-in-pytorch) )\n",
    "\n",
    "### Required libraries ###\n",
    "\n",
    "- ~~Should be the common ML pack we're using. Also with [SD webui's dependency](https://github.com/AUTOMATIC1111/stable-diffusion-webui).~~\n",
    "\n",
    "- [torchview](https://torchview.dev/)\n",
    "- [safetensors](https://huggingface.co/docs/safetensors/index)\n",
    "- [diffusers](https://huggingface.co/docs/diffusers/installation)\n",
    "- [omegaconf](https://anaconda.org/conda-forge/omegaconf)\n",
    "- [pytorch](https://pytorch.org/get-started/locally/#windows-python)\n",
    "- [Graphviz](https://graphviz.org/)\n",
    "- [torchinfo](https://pypi.org/project/torchinfo/)\n",
    "- [prettytable](https://pypi.org/project/prettytable/)\n",
    "- ~~[ultralytics-thop](https://github.com/ultralytics/thop/tree/main)~~\n",
    "\n",
    "### Some layer name to interprept ###\n",
    "\n",
    "- Whole model combined as called `DiffusionPipeline` in [Diffusers](https://huggingface.co/docs/diffusers/index).\n",
    "\n",
    "|Layer name|Description|Class name in Diffusers|\n",
    "|---|---|---|\n",
    "|`first_stage_model`|VAE|`AutoencoderKL`|\n",
    "|`cond_stage_model`|Text Encoder (SD1, SD2)|`CLIPTextModel`|\n",
    "|`conditioner.embedders.0`|Text Encoder 1 (SDXL)|`CLIPTextModel`|\n",
    "|`conditioner.embedders.1`|Text Encoder 2 (SDXL)|`CLIPTextModelWithProjection`|\n",
    "|`model.diffusion_model`|UNET|`UNet2DConditionModel`|\n",
    "|`model_ema`|EMA model for training|n/a|\n",
    "|`cumprod`, `betas`, `alphas`|`CosineAnnealingLR`|n/a|\n",
    "\n",
    "### Some notation (Useful in the bin chart) ###\n",
    "- `attn1`: `sattn` = *Self attention*\n",
    "- `attn2`: `xattn` = *Cross attention*\n",
    "- `ff`: *Feed forward*\n",
    "- `norm`: [Normalisation layer](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html). `elementwise_affine=True` introduces trainable `bias` and `weight`. \n",
    "- `proj`: *Projection*\n",
    "- `emb_layers`: *Embedding layers*\n",
    "- `mlp`: *Multilayer perceptron*\n",
    "- `others`: `ff` + `norm` + `proj` + `emb_layers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'svg'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import diffusers\n",
    "import accelerate\n",
    "import transformers\n",
    "import huggingface_hub\n",
    "from safetensors.torch import load_file #safe_open\n",
    "from diffusers import UNet2DConditionModel, SD3Transformer2DModel, FluxTransformer2DModel, AuraFlowTransformer2DModel, HunyuanDiT2DModel\n",
    "\n",
    "from torchview import draw_graph\n",
    "from torchinfo import summary\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "#from thop import profile\n",
    "\n",
    "import graphviz\n",
    "graphviz.set_jupyter_format('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu124\n",
      "0.31.0\n",
      "4.44.0\n",
      "0.33.0\n",
      "0.24.5\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(diffusers.__version__)\n",
    "print(transformers.__version__)\n",
    "print(accelerate.__version__)\n",
    "print(huggingface_hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for OMP: Error #15\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU for Flux.\n",
    "g_device = \"cuda:0\" #\"cpu\"\n",
    "# Currently for generating graph only.\n",
    "g_seed = 114514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model path\n",
    "model_path = {\n",
    "    \"sd1\": \"runwayml/stable-diffusion-v1-5\",\n",
    "    \"sd2\": \"stabilityai/stable-diffusion-2-1\",\n",
    "    \"sdxl\": \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    \"sd3\": \"stabilityai/stable-diffusion-3-medium-diffusers\",\n",
    "    \"sd35\":\"stabilityai/stable-diffusion-3.5-large\",\n",
    "    \"flux\": \"black-forest-labs/FLUX.1-dev\",\n",
    "    \"af\": \"fal/AuraFlow-v0.2\",\n",
    "    \"hy\": \"Tencent-Hunyuan/HunyuanDiT-Diffusers\",\n",
    "}\n",
    "\n",
    "model_type = torch.float16 if \"cuda\" in g_device else torch.float # CPU doesn't support FP16 / FP8\n",
    "long_type = torch.int64 if \"cuda\" in g_device else torch.long # CPU doesn't support FP16 / FP8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only online model is available.\n",
    "\n",
    "`load_single_file` is failed (versioning hell, omitted). I load online model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_instance = None # Clear\n",
    "unet_instance = {} # Clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We run it later\n",
    "# for k in model_path.keys():\n",
    "#    unet_instance[k] = UNet2DConditionModel.from_pretrained(model_path[k], subfolder=\"unet\",  torch_dtype=torch.float16).to(g_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input size is trial and error.\n",
    "\n",
    "Not actually, we can read `config.json` form the actual official model in HuggingFace, and `nn.Module` has already created with the config. \n",
    "\n",
    "Originally it is scattered in different Git Repos, but HF does a great job here.\n",
    "\n",
    "- `sd1`: [stabilityai/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/unet/config.json)\n",
    "- `sd2`: [stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/unet/config.json)\n",
    "- `sdxl`: [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/unet/config.json)\n",
    "- `sd3`: [stabilityai/stable-diffusion-3-medium-diffusers](https://huggingface.co/stabilityai/stable-diffusion-3-medium-diffusers/blob/main/transformer/config.json)\n",
    "- `flux`: [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/transformer/config.json)\n",
    "- `af`: [fal/AuraFlow-v0.2](https://huggingface.co/fal/AuraFlow-v0.2/blob/main/transformer/config.json)\n",
    "- `hy`: [Tencent-Hunyuan/HunyuanDiT-Diffusers](https://huggingface.co/Tencent-Hunyuan/HunyuanDiT-Diffusers/blob/main/transformer/config.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not Used.\n",
    "input_data_mapping_sample = {\n",
    "    \"sd1\": {\n",
    "        'sample': torch.rand(1, 4, 64, 64).type(model_type).to(g_device),\n",
    "        'timestep': torch.rand(1).type(model_type).to(g_device),\n",
    "        'encoder_hidden_states': torch.rand(1, 77, 768).type(model_type).to(g_device),\n",
    "    },\n",
    "    \"sd2\": {\n",
    "        'sample': torch.rand(1, 4, 96, 96).type(model_type).to(g_device),\n",
    "        'timestep': torch.rand(1).type(model_type).to(g_device),\n",
    "        'encoder_hidden_states': torch.rand(1280, 77, 1024).type(model_type).to(g_device),\n",
    "    },\n",
    "    \"sdxl\": {\n",
    "        'sample': torch.rand(2, 4, 128, 128).type(model_type).to(g_device),\n",
    "        'timestep': torch.rand(1).type(model_type).to(g_device),\n",
    "        'encoder_hidden_states': torch.rand(2, 77, 2048).type(model_type).to(g_device),\n",
    "        'added_cond_kwargs': {\n",
    "            'text_embeds': torch.rand(2, 1280).type(model_type).to(g_device),\n",
    "            'time_ids': torch.rand(2, 6).type(model_type).to(g_device),\n",
    "        },\n",
    "    },\n",
    "    \"sd3\": {\n",
    "        'hidden_states': torch.rand(1, 16, 128, 128).type(model_type).to(g_device),\n",
    "        'timestep': torch.ones((1, )).type(long_type).to(g_device),\n",
    "        'encoder_hidden_states': torch.rand(1, 77, 4096).type(model_type).to(g_device),\n",
    "        'pooled_projections': torch.rand(1, 2048).type(model_type).to(g_device)\n",
    "    },\n",
    "    \"sd35\": {\n",
    "        'hidden_states': torch.rand(1, 16, 128, 128).type(model_type).to(g_device),\n",
    "        'timestep': torch.ones((1, )).type(long_type).to(g_device),\n",
    "        'encoder_hidden_states': torch.rand(1, 77, 4096).type(model_type).to(g_device),\n",
    "        'pooled_projections': torch.rand(1, 2048).type(model_type).to(g_device)\n",
    "    },\n",
    "    \"flux\": {\n",
    "        'hidden_states': torch.rand(1, 4096, 64).type(model_type).to(g_device),\n",
    "        'timestep': torch.ones((1, )).type(long_type).to(g_device),\n",
    "        'guidance': torch.zeros((1, )).type(long_type).to(g_device),\n",
    "        'encoder_hidden_states': torch.rand(1, 256, 4096).type(model_type).to(g_device),\n",
    "        'pooled_projections': torch.rand(1, 768).type(model_type).to(g_device),\n",
    "        'txt_ids': torch.rand(1, 256, 3).type(model_type).to(g_device),\n",
    "        'img_ids': torch.rand(1, 4096, 3).type(model_type).to(g_device)\n",
    "    },\n",
    "    \"af\": {\n",
    "        'hidden_states': torch.rand(2, 4, 128, 128).type(model_type).to(g_device),\n",
    "        'encoder_hidden_states': torch.rand(2, 256, 2048).type(model_type).to(g_device),\n",
    "        'timestep': torch.ones((2, )).type(long_type).to(g_device)\n",
    "    },        \n",
    "    \"hy\": {\n",
    "        'hidden_states': torch.rand(2, 4, 128, 128).type(model_type).to(g_device),\n",
    "        'timestep': torch.ones((2, )).type(long_type).to(g_device),\n",
    "        'encoder_hidden_states': torch.rand(2, 77, 1024).type(model_type).to(g_device),\n",
    "        'text_embedding_mask': torch.rand(2, 77).type(model_type).to(g_device),        \n",
    "        'encoder_hidden_states_t5': torch.rand(2, 256, 2048).type(model_type).to(g_device),\n",
    "        'text_embedding_mask_t5': torch.rand(2, 256).type(model_type).to(g_device),\n",
    "        'image_meta_size': torch.rand(2, 6).type(model_type).to(g_device),\n",
    "        'style': torch.rand((2, )).type(long_type).to(g_device),\n",
    "        # Don't know how it becomes 2D.\n",
    "        'image_rotary_emb': [torch.rand(4096, 88).type(model_type).to(g_device), torch.rand(4096, 88).type(model_type).to(g_device)]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Graph output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_paths = {\n",
    "    #\"sd1\": \"./sd1_unet\",\n",
    "    #\"sd2\": \"./sd2_unet\",\n",
    "    #\"sdxl\": \"./sdxl_unet\", \n",
    "    #\"sd3\": \"./sd3_mmdit\",\n",
    "    \"sd35\": \"./sd35_mmdit\",\n",
    "    #\"flux\": \"./flux_mmdit\", # Good for RTX 3090, 31 minutes on CPU\n",
    "    #\"af\": \"./af_mmdit\",\n",
    "    #\"hy\": \"./hy_mmdit\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For in place generation (requested)\n",
    "png_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the native `torch` approach first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters_native(model):\n",
    "    \n",
    "    #Official approach straight from diffusers.  \n",
    "    diffuser_total_params = model.num_parameters()\n",
    "\n",
    "    #Native approach, along with layer name.\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "\n",
    "    #Another native approach, but in single line\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    result_str = \"\\r\\n\".join([\n",
    "        table.get_string(), \n",
    "        f\"Total Params (diffuser): {str(diffuser_total_params)}\",\n",
    "        f\"Total Trainable Params (torch): {str(total_params)}\", \n",
    "        f\"Total Params (torch): {str(pytorch_total_params)}\"\n",
    "    ])\n",
    "    return result_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main loop. Note that `input_data` is generated inplace. The expected dimension is already available in `model.config` mentioned above.\n",
    "\n",
    "Notice that we need both `depth=1` and `depth=2` to **link MBW layers**.\n",
    "\n",
    "Also it keep the diagram elegent (however the count won't match!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercepted from StableDiffusionXLPipeline\n",
    "# C:\\Users\\User\\.conda\\envs\\sklearn-env\\Lib\\site-packages\\diffusers\\pipelines\\stable_diffusion_xl\n",
    "if False:\n",
    "    #torch.Size([2, 4, 128, 128])\n",
    "    print(latent_model_input.shape)\n",
    "    #torch.Size([])\n",
    "    print(t.shape)\n",
    "    #torch.Size([2, 77, 2048])\n",
    "    print(prompt_embeds.shape)\n",
    "    #torch.Size([2, 1280])\n",
    "    print(added_cond_kwargs[\"text_embeds\"].shape)\n",
    "    #torch.Size([2, 6])\n",
    "    print(added_cond_kwargs[\"time_ids\"].shape)\n",
    "    raise Exception(\"We are here.\")\n",
    "\n",
    "# Intercepted from FluxPipeline\n",
    "# C:\\ProgramData\\Miniconda3\\envs\\novelai-env\\Lib\\site-packages\\diffusers\\pipelines\\flux\n",
    "if False:\n",
    "    #torch.Size([1, 4096, 64])\n",
    "    print(hidden_states.shape)\n",
    "    #tensor([1000.0000,  904.5308,  759.5109,  512.8441], device='cuda:0') / 1000\n",
    "    print(timestep.shape)\n",
    "    #tensor([0.], device='cuda:0')\n",
    "    print(guidance.shape)\n",
    "    #torch.Size([1, 768])\n",
    "    print(pooled_projections.shape)\n",
    "    #torch.Size([1, 256, 4096])\n",
    "    print(encoder_hidden_states.shape)\n",
    "    #torch.Size([1, 256, 3])\n",
    "    print(txt_ids.shape)\n",
    "    #torch.Size([1, 4096, 3])\n",
    "    print(img_ids.shape)\n",
    "    #None\n",
    "    print(joint_attention_kwargs)                \n",
    "    raise Exception(\"We are here.\")\n",
    "\n",
    "# Intercepted from AuraFlowPipeline. Noticed that the variable name is not the one to the DiT.\n",
    "# C:\\ProgramData\\Miniconda3\\envs\\novelai-env\\Lib\\site-packages\\diffusers\\pipelines\\aura_flow\n",
    "if False:\n",
    "    #torch.Size([2, 4, 128, 128])\n",
    "    print(latent_model_input.shape)\n",
    "    #torch.Size([2, 256, 2048])\n",
    "    print(prompt_embeds.shape)\n",
    "    #torch.Size([2])\n",
    "    print(timestep.shape)\n",
    "    raise Exception(\"We are here.\")\n",
    "\n",
    "# Intercepted from HunyuanDiTPipeline. Noticed that the variable name is not the one to the DiT.\n",
    "# C:\\ProgramData\\Miniconda3\\envs\\novelai-env\\Lib\\site-packages\\diffusers\\pipelines\\hunyuandit\n",
    "if False:\n",
    "    #torch.Size([2, 4, 128, 128])  \n",
    "    print(latent_model_input.shape)\n",
    "    #torch.Size([2])\n",
    "    print(t_expand.shape)\n",
    "    #torch.Size([2, 77, 1024])\n",
    "    print(prompt_embeds.shape)\n",
    "    #torch.Size([2, 77])\n",
    "    print(prompt_attention_mask.shape)\n",
    "    #torch.Size([2, 256, 2048])\n",
    "    print(prompt_embeds_2.shape)\n",
    "    #torch.Size([2, 256])\n",
    "    print(prompt_attention_mask_2.shape)\n",
    "    #torch.Size([2, 6])\n",
    "    print(add_time_ids.shape)\n",
    "    #torch.Size([2])\n",
    "    print(style.shape)\n",
    "    #[torch.Size([4096, 88])]\n",
    "    print(image_rotary_emb.shape)\n",
    "    raise Exception(\"We are here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mbw_component(cur_unet):\n",
    "    if cur_unet == \"sd1\":\n",
    "        return UNet2DConditionModel.from_pretrained(model_path[cur_unet], subfolder=\"unet\", torch_dtype=model_type).to(g_device)\n",
    "    elif cur_unet == \"sd2\":\n",
    "        return UNet2DConditionModel.from_pretrained(model_path[cur_unet], subfolder=\"unet\", torch_dtype=model_type).to(g_device)\n",
    "    elif cur_unet == \"sdxl\":\n",
    "        return UNet2DConditionModel.from_pretrained(model_path[cur_unet], subfolder=\"unet\", torch_dtype=model_type).to(g_device)\n",
    "    elif cur_unet == \"sd3\":\n",
    "        return SD3Transformer2DModel.from_pretrained(model_path[cur_unet], subfolder=\"transformer\", torch_dtype=model_type).to(g_device)\n",
    "    elif cur_unet == \"sd35\":\n",
    "        return SD3Transformer2DModel.from_pretrained(model_path[cur_unet], subfolder=\"transformer\", torch_dtype=model_type).to(g_device)\n",
    "    elif cur_unet == \"flux\":\n",
    "        return FluxTransformer2DModel.from_pretrained(model_path[cur_unet], subfolder=\"transformer\", guidance_embeds=True, torch_dtype=model_type).to(g_device)\n",
    "    elif cur_unet == \"af\":\n",
    "        return AuraFlowTransformer2DModel.from_pretrained(model_path[cur_unet], subfolder=\"transformer\", torch_dtype=model_type).to(g_device)\n",
    "    elif cur_unet == \"hy\":\n",
    "        return HunyuanDiT2DModel.from_pretrained(model_path[cur_unet], subfolder=\"transformer\", torch_dtype=model_type).to(g_device)\n",
    "\n",
    "def get_feature_dim(cur_unet):\n",
    "    # Should fit CLIPTextModel.hidden_size, 2048 = 768 + 1280 for SDXL\n",
    "    if cur_unet == \"sd1\":\n",
    "        return unet_instance[cur_unet].config.cross_attention_dim\n",
    "    elif cur_unet == \"sd2\":\n",
    "        return unet_instance[cur_unet].config.cross_attention_dim\n",
    "    elif cur_unet == \"sdxl\":\n",
    "        return unet_instance[cur_unet].config.cross_attention_dim\n",
    "    elif cur_unet == \"sd3\":\n",
    "        return unet_instance[cur_unet].config.joint_attention_dim\n",
    "    elif cur_unet == \"sd35\":\n",
    "        return unet_instance[cur_unet].config.joint_attention_dim\n",
    "    elif cur_unet == \"flux\":\n",
    "        return unet_instance[cur_unet].config.joint_attention_dim\n",
    "    elif cur_unet == \"af\":\n",
    "        return unet_instance[cur_unet].config.joint_attention_dim\n",
    "    elif cur_unet == \"hy\":\n",
    "        return unet_instance[cur_unet].config.cross_attention_dim\n",
    "\n",
    "def get_sample_height(cur_unet):\n",
    "    # IDK why Flux doesn't include the \"128\" as sample_size\n",
    "    if cur_unet == \"sd1\":\n",
    "        return unet_instance[cur_unet].config.sample_size\n",
    "    elif cur_unet == \"sd2\":\n",
    "        return unet_instance[cur_unet].config.sample_size\n",
    "    elif cur_unet == \"sdxl\":\n",
    "        return unet_instance[cur_unet].config.sample_size\n",
    "    elif cur_unet == \"sd3\":\n",
    "        return unet_instance[cur_unet].config.sample_size\n",
    "    elif cur_unet == \"sd35\":\n",
    "        return unet_instance[cur_unet].config.sample_size\n",
    "    elif cur_unet == \"flux\":\n",
    "        # Will drop eventually.\n",
    "        return 64 #default_sample_size\n",
    "    elif cur_unet == \"af\":\n",
    "        return unet_instance[cur_unet].config.sample_size\n",
    "    elif cur_unet == \"hy\":\n",
    "        return unet_instance[cur_unet].config.sample_size\n",
    "    \n",
    "def get_sequence_length(cur_unet):\n",
    "    # IDK why Flux doesn't include the \"128\" as sample_size\n",
    "    if cur_unet == \"sd1\":\n",
    "        return 77 # See CLIPTextModel.max_position_embeddings\n",
    "    elif cur_unet == \"sd2\":\n",
    "        return 77 # See CLIPTextModel.max_position_embeddings\n",
    "    elif cur_unet == \"sdxl\":\n",
    "        return 77 # See CLIPTextModel.max_position_embeddings\n",
    "    elif cur_unet == \"sd3\":\n",
    "        return 77 # See CLIPTextModel.max_position_embeddings\n",
    "    elif cur_unet == \"sd35\":\n",
    "        return 77 # See CLIPTextModel.max_position_embeddings\n",
    "    elif cur_unet == \"flux\":\n",
    "        return 256 #base_seq_len\n",
    "    elif cur_unet == \"af\":\n",
    "        return 256 #max_sequence_length\n",
    "    elif cur_unet == \"hy\":\n",
    "        return unet_instance[cur_unet].config.text_len #77\n",
    "\n",
    "def main_loop(cur_unet):\n",
    "    #240807: It is no longer \"UNet\" but we can still treat it as \"that particular part of diffusion model\"\n",
    "    unet_instance[cur_unet] = get_mbw_component(cur_unet) if cur_unet not in unet_instance else unet_instance[cur_unet]\n",
    "\n",
    "    sequence_length = get_sequence_length(cur_unet)\n",
    "    feature_dim = get_feature_dim(cur_unet)\n",
    "    height = get_sample_height(cur_unet)\n",
    "    width = height #square is fine\n",
    "    channel = unet_instance[cur_unet].config.in_channels\n",
    "    step = 1 #arbitary single float\n",
    "    batch = 1 #1bs\n",
    "\n",
    "    inplace_input_data = {\n",
    "        'sample': torch.rand(batch, channel, height, width).type(model_type).to(g_device),\n",
    "        'timestep': torch.rand(step).type(model_type).to(g_device),\n",
    "        'encoder_hidden_states': torch.rand(batch, sequence_length, feature_dim).type(model_type).to(g_device),\n",
    "    }\n",
    "    \n",
    "    # SDXL special\n",
    "    if cur_unet == \"sdxl\":\n",
    "        addition_time_embed_dim = unet_instance[cur_unet].config.addition_time_embed_dim\n",
    "        projection_class_embeddings_input_dim = unet_instance[cur_unet].config.projection_class_embeddings_input_dim\n",
    "        conv_in_kernel = unet_instance[cur_unet].config.conv_in_kernel\n",
    "        conv_out_kernel = unet_instance[cur_unet].config.conv_out_kernel\n",
    "        time_sequence_length = int((projection_class_embeddings_input_dim - addition_time_embed_dim) / 2)\n",
    "        time_id_length = conv_in_kernel + conv_out_kernel\n",
    "        inplace_input_data['added_cond_kwargs'] = {\n",
    "            'text_embeds': torch.rand(batch, time_sequence_length).type(model_type).to(g_device),\n",
    "            'time_ids': torch.rand(batch, time_id_length).type(model_type).to(g_device),\n",
    "        }\n",
    "\n",
    "    # SD3 special\n",
    "    if (cur_unet == \"sd3\") or (cur_unet == \"sd35\") or (cur_unet == \"flux\") or (cur_unet == \"hy\"):\n",
    "        if not (cur_unet == \"af\"):\n",
    "            projection_dim = unet_instance[cur_unet].config.pooled_projection_dim\n",
    "\n",
    "        # Drop sample\n",
    "        del inplace_input_data['sample']\n",
    "        inplace_input_data['hidden_states'] = torch.rand(batch, channel, height, width).type(model_type).to(g_device)      \n",
    "        inplace_input_data['pooled_projections'] = torch.rand(batch, projection_dim).type(model_type).to(g_device)\n",
    "        inplace_input_data['timestep'] = torch.ones((batch, )).type(long_type).to(g_device) #torch.ones((inner_dim, ), dtype=torch.long).to(g_device)\n",
    "\n",
    "        if (cur_unet == \"flux\"):\n",
    "            #height * width (64*64) instead of feature_dim (4096)\n",
    "            inplace_input_data['hidden_states'] = torch.rand(batch, height * width, channel).type(model_type).to(g_device)\n",
    "            inplace_input_data['guidance'] = torch.zeros((batch, )).type(model_type).to(g_device)\n",
    "            inplace_input_data['txt_ids'] = torch.zeros(batch, sequence_length, 3).type(model_type).to(g_device)\n",
    "            inplace_input_data['img_ids'] = torch.zeros(batch, height * width, 3).type(model_type).to(g_device)\n",
    "\n",
    "        if (cur_unet == \"hy\"):\n",
    "            attention_head_dim = unet_instance[cur_unet].attention_head_dim\n",
    "            sequence_length_t5 = unet_instance[cur_unet].config.text_len_t5\n",
    "            feature_dim_t5 = unet_instance[cur_unet].config.cross_attention_dim_t5\n",
    "            inplace_input_data['encoder_hidden_states_t5'] = torch.rand(batch, sequence_length_t5, feature_dim_t5).type(model_type).to(g_device)\n",
    "            inplace_input_data['text_embedding_mask'] = torch.rand(batch, sequence_length).type(model_type).to(g_device)\n",
    "            inplace_input_data['text_embedding_mask_t5'] = torch.rand(batch, sequence_length_t5).type(model_type).to(g_device)\n",
    "            del inplace_input_data['pooled_projections']\n",
    "            inplace_input_data['timestep'] = torch.ones((batch, )).type(model_type).to(g_device)\n",
    "            inplace_input_data['image_meta_size'] = torch.rand(batch, 6).type(model_type).to(g_device)\n",
    "            inplace_input_data['style'] = torch.rand((batch, )).type(long_type).to(g_device)\n",
    "            # Don't know how it becomes 2D.\n",
    "            inplace_input_data['image_rotary_emb'] = [\n",
    "                torch.rand(feature_dim * 4, attention_head_dim).type(model_type).to(g_device), \n",
    "                torch.rand(feature_dim * 4, attention_head_dim).type(model_type).to(g_device)\n",
    "            ]\n",
    "        \n",
    "    # AF special: just build from scratch.\n",
    "    if (cur_unet == \"af\"):      \n",
    "        do_cfg = 2 # do_classifier_free_guidance\n",
    "        inplace_input_data = {\n",
    "            'hidden_states': torch.rand(batch * do_cfg, channel, height * do_cfg, width * do_cfg).type(model_type).to(g_device),\n",
    "            'encoder_hidden_states': torch.rand(batch * do_cfg, sequence_length, feature_dim).type(model_type).to(g_device),\n",
    "            'timestep': torch.ones((batch * do_cfg, )).type(long_type).to(g_device)\n",
    "        }\n",
    "        \n",
    "    model_summary = summary(unet_instance[cur_unet], \n",
    "        input_data=inplace_input_data, \n",
    "        col_names=(\"input_size\", \"output_size\", \"num_params\")\n",
    "    )\n",
    "\n",
    "    with open(filename_paths[cur_unet] + '.txt', 'w') as the_file:\n",
    "        the_file.write(str(model_summary))\n",
    "\n",
    "    unet_png = draw_graph(unet_instance[cur_unet], \n",
    "        input_data=inplace_input_data, \n",
    "        graph_name=model_path[cur_unet], \n",
    "        device=g_device, mode=\"eval\", \n",
    "        depth=1,     \n",
    "        \n",
    "        roll=True,        \n",
    "        save_graph=True,\n",
    "        filename=filename_paths[cur_unet]\n",
    "    ) #expand_nested=True, hide_inner_tensors=False,   \n",
    "    png_results[cur_unet] = unet_png\n",
    "\n",
    "    unet_png_2 = draw_graph(unet_instance[cur_unet], \n",
    "        input_data=inplace_input_data, \n",
    "        graph_name=model_path[cur_unet], \n",
    "        device=g_device, mode=\"eval\", \n",
    "        depth=2,       \n",
    "        expand_nested=True,\n",
    "        roll=True,        \n",
    "        save_graph=True,\n",
    "        filename='{}_2'.format(filename_paths[cur_unet])\n",
    "    ) #hide_inner_tensors=False, \n",
    "\n",
    "    # Completely disable because of the ugly (*arg)\n",
    "    if False:\n",
    "        thop_input = [v for k, v in inplace_input_data.items()]\n",
    "        macs, params = profile(unet_instance[cur_unet], inputs=(*thop_input, ))\n",
    "        with open(filename_paths[cur_unet] + '.thop.txt', 'w') as the_file:\n",
    "            the_file.write(\"{},{}\".format(str(macs),str(params)))\n",
    "\n",
    "    native_count = count_parameters_native(unet_instance[cur_unet])\n",
    "    with open(filename_paths[cur_unet] + '.native.txt', 'w') as the_file:\n",
    "        the_file.write(str(native_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `sd1`: Should run with no problem\n",
    "- `sd2`: May OOM, however I'm using RTX 3090 now.\n",
    "- `sdxl`: This is tricky: No docuement. [Read code](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py) for workaround.\n",
    "- `sd3`: Login required. Paste the token in `C:\\Users\\User\\.cache\\huggingface\\token`.\n",
    "- `flux`: Same as `sd3`. Also it may be huge AF because I'm using the dev version. CPU mode only! Also requires newest HF libraries! No Doc, only codes in [HF](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformers/transformer_flux.py) and [OG repo](https://github.com/black-forest-labs/flux/blob/main/src/flux/model.py)\n",
    "- `af`: Even less doc then `flux`. Intercept `latent` directly.\n",
    "- `hy`: A bit more doc. However it is also not complete. Intercept `latent` directly also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231df96e75dc43e8977daa5b8d1d6b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Miniconda3\\envs\\novelai-env\\Lib\\site-packages\\diffusers\\models\\attention_processor.py:1101: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  hidden_states = F.scaled_dot_product_attention(query, key, value, dropout_p=0.0, is_causal=False)\n",
      "\n",
      "(process:13056): Pango-WARNING **: 07:08:17.663: couldn't load font \"Linux libertine Not-Rotated 10\", falling back to \"Sans Not-Rotated 10\", expect ugly output.\n",
      "\n",
      "(process:14820): Pango-WARNING **: 07:08:20.921: couldn't load font \"Linux libertine Not-Rotated 10\", falling back to \"Sans Not-Rotated 10\", expect ugly output.\n",
      "dot: graph is too large for cairo-renderer bitmaps. Scaling by 0.715484 to fit\n"
     ]
    }
   ],
   "source": [
    "for k in filename_paths:\n",
    "    main_loop(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate the image inplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#png_results[\"sd1\"].visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#png_results[\"sd2\"].visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#png_results[\"sdxl\"].visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may extend the work below later. Now we block the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: ERROR"
     ]
    }
   ],
   "source": [
    "raise Exception(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all diagrams and summaries are generated. Now we can try to map MBW layers (`IN00-11`, `MID`, `OUT00-11`) to the diagram.\n",
    "\n",
    "Note that local models are used instead. Also only `safetensors` are used.\n",
    "\n",
    "This time models is read as files, and I'm almost OOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_path = {\n",
    "    \"sd1\": \"../../stable-diffusion-webui/tmp/view_unet/21b-AstolfoMix-2020b.safetensors\",\n",
    "    \"sd2\": \"../../stable-diffusion-webui/tmp/view_unet/wd-1-5-beta2-fp16.safetensors\",\n",
    "    \"sdxl\": \"../../stable-diffusion-webui/tmp/view_unet/wdxl-aesthetic-0.9.safetensors\",\n",
    "}\n",
    "\n",
    "local_models = {}\n",
    "\n",
    "for k in local_model_path.keys():\n",
    "    local_models[k] = load_file(local_model_path[k], device='cpu')\n",
    "\n",
    "# I don't even have time to have sneek peek on SDXL models... Let's crack them here\n",
    "max_layers = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_file` only return a HUGE `dict`. You can use JSON library to visualize it, but it is not useful. There is no linkage between the layers.\n",
    "\n",
    "First, make a nice name finder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_a_keyword_in_a_layer(layer, *keywords):\n",
    "    for k in keywords:\n",
    "         if k not in layer:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def do_some_keywords_in_some_layers(model, *keywords):\n",
    "    for layer in list(model.keys()):\n",
    "        if do_a_keyword_in_a_layer(layer, *keywords):\n",
    "            return layer\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read the summary and the png, there are some noticeable layers: \n",
    "\n",
    "|Layer Name|IN / MID / OUT?|Presence of Identifible Layers?|Layer in `sd1` and `sd2`|Layer in SDXL|\n",
    "|---|---|---|---|---|\n",
    "|`Conv2d`|`input_blocks`|n/a|`IN00`|`IN00`|\n",
    "|`CrossAttnDownBlock2D` > `Transformer2DModel`|`input_blocks`|`transformer_blocks`|`IN01`,`IN02`,`IN04`,`IN05`,`IN07`,`IN08`|`IN04`,`IN05`,`IN07`,`IN08`|\n",
    "|`CrossAttnDownBlock2D` > `DownSample2D`|`input_blocks`|`op`|`IN03`,`IN06`,`IN09`|`IN03`,`IN06`|\n",
    "|`DownBlock2D` > `ResnetBlock2D`|`input_blocks`|`in_layers`|`IN10`,`IN11`|`IN01`,`IN02`|\n",
    "|`UNetMidBlock2DCrossAttn`|`middle_block`|`transformer_blocks`|`MID`|`MID`|\n",
    "|`UpBlock2D` > `ResnetBlock2D`|`output_blocks`|`in_layers`|`OUT00`,`OUT01`,`OUT02`|`OUT06`,`OUT07`,`OUT08`|\n",
    "|`CrossAttnUpBlock2D` > `Transformer2DModel`|`output_blocks`|`transformer_blocks`|`OUT03`,`OUT04`,`OUT05`,`OUT06`,`OUT07`,`OUT08`,`OUT09`,`OUT10`,`OUT11`|`OUT00`,`OUT01`,`OUT02`,`OUT03`,`OUT04`,`OUT05`|\n",
    "|`UpBlock2D` > `UpSample2D`|`output_blocks`|`conv`|`OUT02`|n/a|\n",
    "|`CrossAttnUpBlock2D` > `UpSample2D`|`output_blocks`|`conv`|`OUT05`,`OUT08`|`OUT02`,`OUT05`|\n",
    "|`GroupNorm`, `Conv2d`|`output_blocks`|n/a|`OUT`|`OUT`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streadgy ###\n",
    "\n",
    "Note that `block_types` are tested insequence. Larger layers will be indentified first (usually spotted in layer1), then the smaller layers in layer 2, note that most of them has some indentifiers (not unique) to be indentified.\n",
    "\n",
    "### Findings ###\n",
    "\n",
    "IN00, IN03, IN06, IN09 is suprisingly low in layers count.\n",
    "\n",
    "**The Tensor size may not match the summary / png.** It is becase my 64x64 \"image size\" may be compressed into just 3x3 in actual tensor block as in latent space. Also, my \"batch size\" is \"1\" for single image. However it may be \"320 tokens\" after transforming in CLIP. **However 2nd parameter remains constent as shown in most images in the internet.**\n",
    "\n",
    "For most neuron layers, it is $Y=SiLU(WX+B)$ ([SiLU activation](https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html) for [common terms](https://medium.com/@okanyenigun/building-a-neural-network-from-scratch-in-python-a-step-by-step-guide-8f8cab064c8a)), which `weight` is a tensor, and `bias` is a scalar only. We look for tesnsor size.\n",
    "\n",
    "|Model|Size in actual tensor|Size in summary|\n",
    "|---|---|---|\n",
    "|`sd1`|`[320, 4, 3, 3]`|`[1, 4, 64, 64]`|\n",
    "|`sd1`|`[320, 320, 3, 3]`|`[1, 320, 32, 32]`|\n",
    "|`sd1`|`[640, 640, 3, 3]`|`[1, 640, 16, 16]`|\n",
    "|`sd1`|`[1280, 1280, 3, 3]`|`[1, 1280, 8, 8]`|\n",
    "\n",
    "`IN00` doesn't have `op` marked. Also it has `4` channels, therefore it could be `Conv2D`.\n",
    "\n",
    "For `CrossAttnDownBlock2D` in layer1, `DownSample2D` comes after 2x `Transformer2DModel`, resembles `IN03` comes after `IN01` and `IN02` and before `IN04`, which is plausible.\n",
    "\n",
    "Same pattern applies to `DownBlock2D`, then `IN10` and `IN11` are identified.\n",
    "\n",
    "However `UpBlock2D` is a bit different: `UpSample2D` doesn't follow `DownSample2D`, it has `conv` instead of `op`, and the id is different. \n",
    "\n",
    "`OUT11` doesn't have `UpSample2D` even it is `CrossAttnUpBlock2D` in layer 1. *What an asymmetry*.\n",
    "\n",
    "`OUT` has no suffix like `IN00` also. However `OUT` has *2 distinct layers* with same size: `GroupNorm` and  `Conv2d`.\n",
    "\n",
    "SD2 is almost identical with SD1, with only slight size in difference.\n",
    "\n",
    "SDXL is **different** with SD1. The greatest difference is `UpBlock2D` is now **after** `CrossAttnUpBlock2D` instead of before them. Also `UpBlock2D` doesn't have `UpSample2D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_interested_layers(model, layers_count = max_layers):\n",
    "    block_layers = ['input_blocks.0','input_blocks','middle_block','output_blocks','.out']\n",
    "    block_types = ['transformer_blocks','in_layers','conv','op']\n",
    "\n",
    "    for b_l in block_layers:\n",
    "        for i in range(layers_count):\n",
    "            # Search for direct layers (not much)\n",
    "            direct_layer = do_some_keywords_in_some_layers(model, \"{}.{}.{}\".format(b_l, i, 'weight'))\n",
    "            if direct_layer:\n",
    "                print('{}: {}'.format(direct_layer, model[direct_layer].size()))\n",
    "            else:\n",
    "                for b_t in block_types:\n",
    "                    serarch_result = do_some_keywords_in_some_layers(model, \"{}.{}.\".format(b_l, i), b_t)\n",
    "                    if serarch_result:\n",
    "                        print('{}: {}'.format(serarch_result, model[serarch_result].size()))\n",
    "                        break\n",
    "                # Special case: UpSample2D are embedded in some output_blocks\n",
    "                if (b_l == 'output_blocks'):\n",
    "                    is_upsample2d = do_some_keywords_in_some_layers(model, \"{}.{}.\".format(b_l, i), 'conv')\n",
    "                    if is_upsample2d:\n",
    "                        print('{}: {}'.format(is_upsample2d, model[is_upsample2d].size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.diffusion_model.input_blocks.0.0.weight: torch.Size([320, 4, 3, 3])\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.input_blocks.3.0.op.bias: torch.Size([320])\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.input_blocks.6.0.op.bias: torch.Size([640])\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.input_blocks.9.0.op.bias: torch.Size([1280])\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.middle_block.0.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.middle_block.2.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.0.bias: torch.Size([2560])\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.0.bias: torch.Size([2560])\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.0.bias: torch.Size([2560])\n",
      "model.diffusion_model.output_blocks.2.1.conv.bias: torch.Size([1280])\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.output_blocks.5.2.conv.bias: torch.Size([1280])\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.output_blocks.8.2.conv.bias: torch.Size([640])\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.out.0.weight: torch.Size([320])\n",
      "model.diffusion_model.out.2.weight: torch.Size([4, 320, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "list_interested_layers(local_models['sd1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.diffusion_model.input_blocks.0.0.weight: torch.Size([320, 4, 3, 3])\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.input_blocks.3.0.op.bias: torch.Size([320])\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.input_blocks.6.0.op.bias: torch.Size([640])\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.input_blocks.9.0.op.bias: torch.Size([1280])\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.middle_block.0.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.middle_block.2.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.0.bias: torch.Size([2560])\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.0.bias: torch.Size([2560])\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.0.bias: torch.Size([2560])\n",
      "model.diffusion_model.output_blocks.2.1.conv.bias: torch.Size([1280])\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.output_blocks.5.2.conv.bias: torch.Size([1280])\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.output_blocks.8.2.conv.bias: torch.Size([640])\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([320, 320])\n",
      "model.diffusion_model.out.0.weight: torch.Size([320])\n",
      "model.diffusion_model.out.2.weight: torch.Size([4, 320, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "list_interested_layers(local_models['sd2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.diffusion_model.input_blocks.0.0.weight: torch.Size([320, 4, 3, 3])\n",
      "model.diffusion_model.input_blocks.1.0.in_layers.0.bias: torch.Size([320])\n",
      "model.diffusion_model.input_blocks.2.0.in_layers.0.bias: torch.Size([320])\n",
      "model.diffusion_model.input_blocks.3.0.op.bias: torch.Size([320])\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.input_blocks.6.0.op.bias: torch.Size([640])\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.middle_block.0.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.middle_block.2.in_layers.0.bias: torch.Size([1280])\n",
      "model.diffusion_model.output_blocks.0.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.output_blocks.1.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.output_blocks.2.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([1280, 1280])\n",
      "model.diffusion_model.output_blocks.2.2.conv.bias: torch.Size([1280])\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight: torch.Size([640, 640])\n",
      "model.diffusion_model.output_blocks.5.2.conv.bias: torch.Size([640])\n",
      "model.diffusion_model.output_blocks.6.0.in_layers.0.bias: torch.Size([960])\n",
      "model.diffusion_model.output_blocks.7.0.in_layers.0.bias: torch.Size([640])\n",
      "model.diffusion_model.output_blocks.8.0.in_layers.0.bias: torch.Size([640])\n",
      "model.diffusion_model.out.0.weight: torch.Size([320])\n",
      "model.diffusion_model.out.2.weight: torch.Size([4, 320, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "list_interested_layers(local_models['sdxl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "novelai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
