{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising UNET #\n",
    "\n",
    "### Abstract ###\n",
    "\n",
    "- Self explained. Using `torch_view` as main library.\n",
    "- The docuement below is mainly copied from [mega_cmp.ipynb](./v2a/mmega_cmp.ipynb)\n",
    "\n",
    "### Required libraries ###\n",
    "\n",
    "- ~~Should be the common ML pack we're using. Also with [SD webui's dependency](https://github.com/AUTOMATIC1111/stable-diffusion-webui).~~\n",
    "\n",
    "- [torchview](https://torchview.dev/)\n",
    "- [scikit-learn](https://scikit-learn.org/stable/install.html)\n",
    "- [NetworkX](https://networkx.org/documentation/stable/release/release_3.0.html)\n",
    "- [safetensors](https://huggingface.co/docs/safetensors/index)\n",
    "- [diffusers](https://huggingface.co/docs/diffusers/installation)\n",
    "- [omegaconf](https://anaconda.org/conda-forge/omegaconf)\n",
    "- [pytorch](https://pytorch.org/get-started/locally/#windows-python)\n",
    "- [matplotlib](https://matplotlib.org/stable/api/matplotlib_configuration_api.html)\n",
    "- [numpy](https://numpy.org/)\n",
    "- [torchinfo](https://pypi.org/project/torchinfo/)\n",
    "\n",
    "### Some layer name to interprept (for SD1.5) ###\n",
    "- `first_stage_model`: VAE\n",
    "- `cond_stage_model`: Text Encoder\n",
    "- `model.diffusion_model`: Diffusion model\n",
    "- `model_ema`: EMA model for training\n",
    "- `cumprod`, `betas`, `alphas`: `CosineAnnealingLR`\n",
    "\n",
    "### Some notation (Useful in the bin chart) ###\n",
    "- `attn1`: `sattn` = *Self attention*\n",
    "- `attn2`: `xattn` = *Cross attention*\n",
    "- `ff`: *Feed forward*\n",
    "- `norm`: [Normalisation layer](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html). `elementwise_affine=True` introduces trainable `bias` and `weight`. \n",
    "- `proj`: *Projection*\n",
    "- `emb_layers`: *Embedding layers*\n",
    "- `others`: `ff` + `norm` + `proj` + `emb_layers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'svg'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from safetensors.torch import load_file #safe_open\n",
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "from torchview import draw_graph\n",
    "from torchinfo import summary\n",
    "\n",
    "import graphviz\n",
    "graphviz.set_jupyter_format('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu118'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for OMP: Error #15\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Support 'cuda', but 'cpu' is arleady fast.\n",
    "g_device = \"cuda:0\" #\"cpu\"\n",
    "# Currently for generating graph only.\n",
    "g_seed = 114514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model path\n",
    "model_path = {\n",
    "    \"sd1\": \"runwayml/stable-diffusion-v1-5\",\n",
    "    \"sd2\": \"stabilityai/stable-diffusion-2-1\",\n",
    "    \"sdxl\": \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "}\n",
    "\n",
    "model_type = torch.float16 if \"cuda\" in g_device else torch.float # CPU doesn't support FP16 / FP8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only online model is available.\n",
    "\n",
    "`load_single_file` is failed (versioning hell, omitted). I load online model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_instance = None # Clear\n",
    "unet_instance = {} # Clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We run it later\n",
    "# for k in model_path.keys():\n",
    "#    unet_instance[k] = UNet2DConditionModel.from_pretrained(model_path[k], subfolder=\"unet\",  torch_dtype=torch.float16).to(g_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input size is trial and error.\n",
    "\n",
    "Not actually, we can read `config.json` form the actual official model in HuggingFace, and `nn.Module` has already created with the config. \n",
    "\n",
    "Originally it is scattered in different Git Repos, but HF does a great job here.\n",
    "\n",
    "- `sd1`: [stabilityai/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/unet/config.json)\n",
    "- `sd2`: [stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/unet/config.json)\n",
    "- `sdxl`: [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/unet/config.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not Used.\n",
    "input_data_mapping_sample = {\n",
    "    \"sd1\": {\n",
    "        'sample': torch.rand(1, 4, 64, 64, dtype=model_type).to(g_device),\n",
    "        'timestep': torch.rand(1, dtype=model_type).to(g_device),\n",
    "        'encoder_hidden_states': torch.rand(1, 77, 768, dtype=model_type).to(g_device),\n",
    "    },\n",
    "    \"sd2\": {\n",
    "        'sample': torch.rand(1, 4, 96, 96, dtype=model_type).to(g_device),\n",
    "        'timestep': torch.rand(1, dtype=model_type).to(g_device),\n",
    "        'encoder_hidden_states': torch.rand(1280, 77, 1024, dtype=model_type).to(g_device),\n",
    "    },\n",
    "    \"sdxl\": {\n",
    "        'sample': torch.rand(1, 4, 128, 128, dtype=model_type).to(g_device),\n",
    "        'timestep': torch.rand(1, dtype=model_type).to(g_device),\n",
    "        'encoder_hidden_states': torch.rand(1280, 77, 2048, dtype=model_type).to(g_device),\n",
    "        'added_cond_kwargs': {\n",
    "            'text_embeds': torch.rand(1280, 2560, dtype=model_type).to(g_device),\n",
    "            'time_ids': torch.rand(1280, dtype=model_type).to(g_device),\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Graph output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_paths = {\n",
    "    \"sd1\": \"./sd1_unet\",\n",
    "    \"sd2\": \"./sd2_unet\",\n",
    "    \"sdxl\": \"./sdxl_unet\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main loop. Note that `input_data` is generated inplace. The expected dimension is already available in `model.config` mentioned above.\n",
    "\n",
    "Notice that we keep `depth=1` since we are desperate to **link MBW layers** instead of study how the UNET layers work (there are plenty of guide).\n",
    "\n",
    "Also it keep the diagram elegent (however the count won't match!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop(cur_unet):\n",
    "    unet_instance[cur_unet] = UNet2DConditionModel.from_pretrained(model_path[cur_unet], subfolder=\"unet\",  torch_dtype=model_type).to(g_device) if cur_unet not in unet_instance else unet_instance[cur_unet]\n",
    "\n",
    "    channel = unet_instance[cur_unet].config.in_channels\n",
    "    sequence_length = 77 # See CLIPTextModel.max_position_embeddings\n",
    "    feature_dim = unet_instance[cur_unet].config.cross_attention_dim # Should fit CLIPTextModel.hidden_size, 2048 = 768 + 1280 for SDXL\n",
    "    height = unet_instance[cur_unet].config.sample_size\n",
    "    width = height #square is fine\n",
    "    step = 1 #arbitary single float\n",
    "    batch = 1 #1bs\n",
    "\n",
    "    inplace_input_data = {\n",
    "        'sample': torch.rand(batch, channel, height, width, dtype=model_type).to(g_device),\n",
    "        'timestep': torch.rand(step, dtype=model_type).to(g_device),\n",
    "        'encoder_hidden_states': torch.rand(batch, sequence_length, feature_dim, dtype=model_type).to(g_device),\n",
    "    }\n",
    "\n",
    "    # SDXL special\n",
    "    if cur_unet == \"sdxl\":\n",
    "        addition_time_embed_dim = unet_instance[cur_unet].config.addition_time_embed_dim\n",
    "        projection_class_embeddings_input_dim = unet_instance[cur_unet].config.projection_class_embeddings_input_dim\n",
    "        time_sequence_length = projection_class_embeddings_input_dim - addition_time_embed_dim\n",
    "        inplace_input_data['added_cond_kwargs'] = {\n",
    "            'text_embeds': torch.rand(batch, time_sequence_length, dtype=model_type).to(g_device),\n",
    "            'time_ids': torch.rand(batch, dtype=model_type).to(g_device),\n",
    "        }\n",
    "\n",
    "    model_summary = summary(unet_instance[cur_unet], \n",
    "        input_data=inplace_input_data, \n",
    "        col_names=(\"input_size\", \"output_size\", \"num_params\")\n",
    "    )\n",
    "\n",
    "    with open(filename_paths[cur_unet] + '.txt', 'w') as the_file:\n",
    "        the_file.write(str(model_summary))\n",
    "\n",
    "    unet_png = draw_graph(unet_instance[cur_unet], \n",
    "        input_data=inplace_input_data, \n",
    "        graph_name=model_path[cur_unet], \n",
    "        device=g_device, mode=\"eval\", \n",
    "        depth=1,       \n",
    "        roll=True,        \n",
    "        save_graph=True,\n",
    "        filename=filename_paths[cur_unet]\n",
    "    ) #, expand_nested=True, hide_inner_tensors=False, \n",
    "\n",
    "    #unet_png.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `sd1`: Should run with no problem\n",
    "- `sd2`: May OOM, however I'm using RTX 3090 now.\n",
    "- `sdxl`: This is tricky: No docuement. [Read code](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py) for workaround."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "\n",
      "(process:11512): Pango-WARNING **: 01:21:51.031: couldn't load font \"Linux libertine Not-Rotated 10\", falling back to \"Sans Not-Rotated 10\", expect ugly output.\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "\n",
      "(process:4780): Pango-WARNING **: 01:22:03.009: couldn't load font \"Linux libertine Not-Rotated 10\", falling back to \"Sans Not-Rotated 10\", expect ugly output.\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "\n",
      "(process:36120): Pango-WARNING **: 01:22:36.308: couldn't load font \"Linux libertine Not-Rotated 10\", falling back to \"Sans Not-Rotated 10\", expect ugly output.\n"
     ]
    }
   ],
   "source": [
    "for k in filename_paths:\n",
    "    main_loop(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all diagrams and summaries are generated. Now we can try to map MBW layers (`IN00-11`, `MID`, `OUT00-11`) to the diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_path = \"../../stable-diffusion-webui/tmp/view_unet/21b-AstolfoMix-2020b.safetensors\"\n",
    "local_model = load_file(local_model_path, device=g_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_file` only return a HUGE `dict`. You can use JSON library to visualize it, but it is not useful. There is no linkage between the layers.\n",
    "\n",
    "First, make a nice name finder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_a_keyword_in_a_layer(layer, *keywords):\n",
    "    for k in keywords:\n",
    "         if k not in layer:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def do_some_keywords_in_some_layers(model, *keywords):\n",
    "    for layer in list(model.keys()):\n",
    "        if do_a_keyword_in_a_layer(layer, *keywords):\n",
    "            return layer\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read the summary and the png, there are some noticeable layers: \n",
    "\n",
    "|Layer Name|IN / MID / OUT?|Presence of Transformer Layers?|\n",
    "|---|---|---|\n",
    "|`CrossAttnDownBlock2D`|`input_blocks`|`transformer_blocks`| \n",
    "|`DownBlock2D`|`input_blocks`|n/a|\n",
    "|`UNetMidBlock2DCrossAttn`|`middle_block`|`transformer_blocks`|\n",
    "|`UpBlock2D`|`output_blocks`|n/a|\n",
    "|`CrossAttnUpBlock2D`|`output_blocks`|`transformer_blocks`|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight\n"
     ]
    }
   ],
   "source": [
    "block_layers = ['input_blocks', 'middle_block', 'output_blocks']\n",
    "layers_count = 12\n",
    "\n",
    "for b_l in block_layers:\n",
    "    for i in range(layers_count):\n",
    "        serarch_result = do_some_keywords_in_some_layers(local_model, \"{}.{}.\".format(b_l, i), \"transformer_blocks\")\n",
    "        if serarch_result:\n",
    "            print(serarch_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
