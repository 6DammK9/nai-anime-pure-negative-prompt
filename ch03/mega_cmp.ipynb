{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "019fd4e3-4c88-43a3-a2bb-3ec4c7d0fa92",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Mega similarity check #\n",
    "\n",
    "### Abstract ###\n",
    "\n",
    "- Distance metric is not meaningful, **but the identity does.**\n",
    "- However I still prefer L2 distance ~~because it looks like I'm doing a ML task~~.\n",
    "- Objective: Try to explain *subjective* experience with model difference, especially if any components have been **changed**, ignoring how much it has been changed.\n",
    "- Thanks [\"RC\"](https://github.com/CCRcmcpe) for providing the initial script (and the idea).\n",
    "\n",
    "### Input ### \n",
    "- See next cell. Paths of models and abbreviation you like.\n",
    "\n",
    "### Output ###\n",
    "- TONS of JSON, showing `(layer_name, distance_between_2_models)`\n",
    "\n",
    "### Special case or comparasion ###\n",
    "- Text encoder for model `nai`: `\"cond_stage_model.transformer\", \"cond_stage_model.transformer.text_model\"`\n",
    "\n",
    "### Some layer name to interprept ###\n",
    "- `first_stage_model`: VAE\n",
    "- `cond_stage_model`: Text Encoder\n",
    "- `model.diffusion_model`: Diffusion model\n",
    "- `model_ema`: EMA model for training\n",
    "- `cumprod`, `betas`, `alphas`: `CosineAnnealingLR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6714cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths here.\n",
    "ofp_folder = \"json/\"\n",
    "cmp_mapping = [\n",
    "    [\n",
    "        \"vae\", [\n",
    "            [\n",
    "                [\"full\", \"../../stable-diffusion-webui/tmp/VAE/kl-f8-full.ckpt\"],\n",
    "                [\"wd1\", \"../../stable-diffusion-webui/tmp/VAE/kl-f8-anime.ckpt\"]\n",
    "            ],\n",
    "            [\n",
    "                [\"full\", \"../../stable-diffusion-webui/tmp/VAE/kl-f8-full.ckpt\"],\n",
    "                [\"sd84k\", \"../../stable-diffusion-webui/tmp/VAE/vae-ft-mse-840000-ema-pruned.ckpt\"]\n",
    "            ],\n",
    "            [\n",
    "                [\"wd1\", \"../../stable-diffusion-webui/tmp/VAE/kl-f8-anime.ckpt\"],\n",
    "                [\"wd2\", \"../../stable-diffusion-webui/tmp/VAE/kl-f8-anime2.ckpt\"]\n",
    "            ],\n",
    "            [\n",
    "                [\"sd84k\", \"../../stable-diffusion-webui/tmp/VAE/vae-ft-mse-840000-ema-pruned.ckpt\"],\n",
    "                [\"wd2\", \"../../stable-diffusion-webui/tmp/VAE/kl-f8-anime2.ckpt\"]\n",
    "            ],\n",
    "            [\n",
    "                [\"sd84k\", \"../../stable-diffusion-webui/tmp/VAE/vae-ft-mse-840000-ema-pruned.ckpt\"],\n",
    "                [\"nai\", \"../../stable-diffusion-webui/tmp/VAE/animevae.pt\"]\n",
    "            ],\n",
    "            [\n",
    "                [\"wd2\", \"../../stable-diffusion-webui/tmp/VAE/kl-f8-anime2.ckpt\"],\n",
    "                [\"nai\", \"../../stable-diffusion-webui/tmp/VAE/animevae.pt\"]\n",
    "            ]\n",
    "        ]            \n",
    "    ],\n",
    "    [\n",
    "        \"sd1\", [\n",
    "            [\n",
    "                [\"sd12\", \"../../stable-diffusion-webui/models/Stable-diffusion/sd-v1-2-full-ema.ckpt\"],\n",
    "                [\"sd14\", \"../../stable-diffusion-webui/models/Stable-diffusion/sd-v1-4-full-ema.ckpt\"]\n",
    "            ]\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        \"nai\", [           \n",
    "            [\n",
    "                [\"sd147g\", \"../../stable-diffusion-webui/models/Stable-diffusion/sd-v1-4-full-ema.ckpt\"],\n",
    "                [\"nai7g\", \"../../stable-diffusion-webui/tmp/nodelaileak/animefull-latest.ckpt\"]\n",
    "            ],\n",
    "            [\n",
    "                [\"sd144g\", \"../../stable-diffusion-webui/models/Stable-diffusion/sd-v1-4.ckpt\"],\n",
    "                [\"nai4g\", \"../../stable-diffusion-webui/models/Stable-diffusion/animefull-final-pruned.ckpt\"]\n",
    "            ],\n",
    "            [\n",
    "                [\"nai7g\", \"../../stable-diffusion-webui/tmp/nodelaileak/animefull-latest.ckpt\"],\n",
    "                [\"nai4g\", \"../../stable-diffusion-webui/models/Stable-diffusion/animefull-final-pruned.ckpt\"]\n",
    "            ],\n",
    "            [        \n",
    "                [\"sd144g\", \"../../stable-diffusion-webui/models/Stable-diffusion/sd-v1-4.ckpt\"],\n",
    "                [\"ac\", \"../../stable-diffusion-webui/models/Stable-diffusion/ACertainty.ckpt\"]\n",
    "            ]\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        \"any3\", [\n",
    "            [\n",
    "                [\"nai4g\", \"../../stable-diffusion-webui/models/Stable-diffusion/animefull-final-pruned.ckpt\"],\n",
    "                [\"any3\", \"../../stable-diffusion-webui/models/Stable-diffusion/Anything-V3.0-pruned-fp32.ckpt\"]\n",
    "            ],\n",
    "            [\n",
    "                [\"mmke\", \"../../stable-diffusion-webui/models/Stable-diffusion/momoko-e.ckpt\"],\n",
    "                [\"any3\", \"../../stable-diffusion-webui/models/Stable-diffusion/Anything-V3.0-pruned-fp32.ckpt\"]\n",
    "            ], \n",
    "            [\n",
    "                [\"nai4g\", \"../../stable-diffusion-webui/models/Stable-diffusion/animefull-final-pruned.ckpt\"],\n",
    "                [\"mmke\", \"../../stable-diffusion-webui/models/Stable-diffusion/momoko-e.ckpt\"]\n",
    "            ],      \n",
    "            [\n",
    "                [\"any3\", \"../../stable-diffusion-webui/models/Stable-diffusion/Anything-V3.0-pruned-fp32.ckpt\"],\n",
    "                [\"basil\", \"../../stable-diffusion-webui/models/Stable-diffusion/basil_mix.ckpt\"]\n",
    "            ],\n",
    "            [\n",
    "                [\"any3\", \"../../stable-diffusion-webui/models/Stable-diffusion/Anything-V3.0-pruned-fp32.ckpt\"],\n",
    "                [\"any4\", \"../../stable-diffusion-webui/models/Stable-diffusion/anything-v4.0-pruned.safetensors\"]\n",
    "            ]\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        \"nmfsan\", [\n",
    "            [  \n",
    "                [\"ao\", \"../../stable-diffusion-webui/models/Stable-diffusion/sample-nd-epoch59.safetensors\"],\n",
    "                [\"cc\", \"../../stable-diffusion-webui/models/Stable-diffusion/bp_nman_e29.safetensors\"]\n",
    "            ],\n",
    "            [  \n",
    "                [\"any3\", \"../../stable-diffusion-webui/models/Stable-diffusion/Anything-V3.0-pruned-fp32.ckpt\"],\n",
    "                [\"cc\", \"../../stable-diffusion-webui/models/Stable-diffusion/bp_nman_e29.safetensors\"]\n",
    "            ]\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        \"sd2\", [\n",
    "            [  \n",
    "                [\"sd14\", \"../../stable-diffusion-webui/models/Stable-diffusion/sd-v1-4.ckpt\"],\n",
    "                [\"sd21\", \"../../stable-diffusion-webui/tmp/SD2/sd-21-768-ema-pruned.ckpt\"]\n",
    "            ],\n",
    "            [  \n",
    "                [\"sd20\", \"../../stable-diffusion-webui/tmp/SD2/sd-20-768-v-ema.ckpt\"],\n",
    "                [\"sd21\", \"../../stable-diffusion-webui/tmp/SD2/sd-21-768-ema-pruned.ckpt\"]\n",
    "            ],\n",
    "            [  \n",
    "                [\"sd20\", \"../../stable-diffusion-webui/tmp/SD2/sd-20-768-v-ema.ckpt\"],\n",
    "                [\"wd14\", \"../../stable-diffusion-webui/tmp/SD2/wd-1-4-anime_e1.ckpt\"]\n",
    "            ],\n",
    "            [  \n",
    "                [\"sd21\", \"../../stable-diffusion-webui/tmp/SD2/sd-21-768-ema-pruned.ckpt\"],\n",
    "                [\"cjd211\", \"../../stable-diffusion-webui/tmp/SD2/cjd-v2-1-1.safetensors\"]\n",
    "            ],\n",
    "            [  \n",
    "                [\"wd14\", \"../../stable-diffusion-webui/tmp/SD2/wd-1-4-anime_e1.ckpt\"],\n",
    "                [\"p1at\", \"../../stable-diffusion-webui/tmp/SD2/wd-1-4-sd-2-1-025-text-encoder.ckpt\"]\n",
    "            ],\n",
    "            [  \n",
    "                [\"wd14e1\", \"../../stable-diffusion-webui/tmp/SD2/wd-1-4-anime_e1.ckpt\"],\n",
    "                [\"wd14e2\", \"../../stable-diffusion-webui/tmp/SD2/wd-1-4-anime_e2.ckpt\"]\n",
    "            ],\n",
    "            [  \n",
    "                [\"sd21\", \"../../stable-diffusion-webui/tmp/SD2/sd-21-768-ema-pruned.ckpt\"],\n",
    "                [\"jrd\", \"../../stable-diffusion-webui/tmp/SD2/sd2_1_ucg_autotagger_ruminant_notte_12_21.ckpt\"]\n",
    "            ]        \n",
    "        ]  \n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6517a1-1515-415c-819d-5b496e5676e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import click\n",
    "import torch\n",
    "import json \n",
    "from safetensors.torch import load_file\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "300ccc5b-4363-4002-8018-30e72fd0fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8ee09f7-dcac-4e7f-908e-5fb14bdd9cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path: Path, device: str, print_ptl_info=False) -> dict[str, torch.Tensor]:\n",
    "    if \".safetensors\" in path.suffixes:\n",
    "        return load_file(path, device=device)\n",
    "    else:\n",
    "        ckpt = torch.load(path, map_location=device)\n",
    "        if print_ptl_info and \"epoch\" in ckpt and \"global_step\" in ckpt:\n",
    "            print(f\"[I] {path.name}: epoch {ckpt['epoch']}, step {ckpt['global_step']}\")\n",
    "        return ckpt[\"state_dict\"] if \"state_dict\" in ckpt else ckpt\n",
    "\n",
    "\n",
    "def check_equal_shape(a: torch.Tensor, b: torch.Tensor, fn):\n",
    "    if a.shape != b.shape:\n",
    "        return \"nan\" #\"DIFFERENT SHAPE\"\n",
    "\n",
    "    return fn(a.type(torch.float),b.type(torch.float))\n",
    "    #return fn(a.reshape(-1), b.reshape(-1))\n",
    "\n",
    "\n",
    "METRIC_MAP = {\n",
    "    \"equal\": torch.equal,\n",
    "    \"l1\": lambda a, b: check_equal_shape(a, b, lambda a, b: torch.dist(a, b, p=1)),\n",
    "    \"l2\": lambda a, b: check_equal_shape(a, b, lambda a, b: torch.dist(a, b, p=2)),\n",
    "    \"cossim\": lambda a, b: check_equal_shape(a, b, lambda a, b: torch.mean(torch.cosine_similarity(a, b, dim=0)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ef0b25b-61e4-4bfb-8bcc-cbbf8e148753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp_c(a_path, b_path, device, metric, no_ptl_info):\n",
    "    metric_fn = METRIC_MAP[metric]\n",
    "    \n",
    "    try:\n",
    "        a_path = a_path.decode('UTF-8')\n",
    "        b_path = b_path.decode('UTF-8')\n",
    "    except:\n",
    "        #No need\n",
    "        pass\n",
    "\n",
    "    a = load_model(Path(a_path), device, not no_ptl_info)\n",
    "    b = load_model(Path(b_path), device, not no_ptl_info)\n",
    "\n",
    "    ak = set(a.keys())\n",
    "    bk = set(b.keys())\n",
    "    \n",
    "    keys_inter = ak.intersection(bk)\n",
    "    da = list(ak.difference(bk))\n",
    "    db = list(bk.difference(ak))\n",
    "    kv = {}\n",
    "    for k in keys_inter:\n",
    "        rt = metric_fn(a[k], b[k])\n",
    "        try:\n",
    "           rt =  rt.numpy().tolist()\n",
    "        except:\n",
    "            #\"nan\" or True / False\n",
    "            pass\n",
    "        kv[k] = rt\n",
    "\n",
    "    #Special case: NAI renamed the TE (claimed using GPT-2)\n",
    "    if not ((\"animefull\" in str(a_path)) and (\"animefull\" in str(b_path))):\n",
    "        if \"animefull\" in str(a_path):\n",
    "            for dak in da:\n",
    "                if \"cond_stage_model.transformer\" in dak:\n",
    "                    kv[\"nai.\" + dak] = metric_fn(a[dak], b[dak.replace(\"cond_stage_model.transformer\", \"cond_stage_model.transformer.text_model\")]).numpy().tolist()\n",
    "        elif \"animefull\" in str(b_path):\n",
    "            for dbk in db:\n",
    "                if \"cond_stage_model.transformer\" in dbk:\n",
    "                    kv[\"nai.\" + dbk] = metric_fn(b[dbk], a[dbk.replace(\"cond_stage_model.transformer\", \"cond_stage_model.transformer.text_model\")]).numpy().tolist()\n",
    "\n",
    "    return kv, da, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cc121bc-e1c9-4e0a-84b9-ad2e50eb4cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp_json(pa,pb,ofp,d=\"l2\",npi=True):\n",
    "    kv, da, db = cmp_c(Path(pa), Path(pb), g_device, d, npi)\n",
    "    dj = {'kv':kv, 'da':da, 'db':db}\n",
    "    with open(ofp, \"w\") as outfile:\n",
    "        json.dump(dj, outfile, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55067342-8567-472c-9a07-dbbb4b210c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing: Obvious result\n",
    "#cmp_json(\"./cosmoany.safetensors\", \"./cosmoany.safetensors\", \"json/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a299e852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json/vae_full_wd1.json\n",
      "json/vae_full_sd84k.json\n",
      "json/vae_wd1_wd2.json\n",
      "json/vae_sd84k_wd2.json\n",
      "json/vae_sd84k_nai.json\n",
      "json/vae_wd2_nai.json\n",
      "json/sd1_sd12_sd14.json\n",
      "json/nai_sd147g_nai7g.json\n",
      "json/nai_sd144g_nai4g.json\n",
      "json/nai_nai7g_nai4g.json\n",
      "json/nai_sd144g_ac.json\n",
      "json/any3_nai4g_any3.json\n",
      "json/any3_mmke_any3.json\n",
      "json/any3_nai4g_mmke.json\n",
      "json/any3_any3_basil.json\n",
      "json/any3_any3_any4.json\n",
      "json/nmfsan_ao_cc.json\n",
      "json/nmfsan_any3_cc.json\n",
      "json/sd2_sd14_sd21.json\n",
      "json/sd2_sd20_sd21.json\n",
      "json/sd2_sd20_wd14.json\n",
      "json/sd2_sd21_cjd211.json\n",
      "json/sd2_wd14_p1at.json\n",
      "json/sd2_wd14e1_wd14e2.json\n",
      "json/sd2_sd21_jrd.json\n",
      "Compare: 25, time: 148 sec\n"
     ]
    }
   ],
   "source": [
    "cmp_count = 0\n",
    "ts = time.time()\n",
    "for cm0 in cmp_mapping:\n",
    "    ofp0 = cm0[0]\n",
    "    for pab in cm0[1]: \n",
    "        pak = pab[0][0]\n",
    "        pav = pab[0][1]\n",
    "        pbk = pab[1][0]\n",
    "        pbv = pab[1][1]\n",
    "        ofp = \"{}{}_{}_{}.json\".format(ofp_folder, ofp0, pak, pbk)\n",
    "        print(ofp)\n",
    "        cmp_count = cmp_count + 1\n",
    "        cmp_json(pav, pbv, ofp)\n",
    "te = time.time()\n",
    "print(\"Compare: {}, time: {} sec\".format(cmp_count, int(te - ts)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "503b9db4-4a84-4988-a51e-31493502beeb",
   "metadata": {},
   "source": [
    "### Findings (VAE) ###\n",
    "- `kl-f8` vs SD prune: Some layers are pruned.\n",
    "- `kl-f8` vs WD1: Both `encoder`, `decoder` is trained\n",
    "- WD1 vs WD2: Only `decoder` is trained\n",
    "- `kl-f8` vs NAI: Only `decoder` is trained. **However it is same as SD v1.4 bundled. See below.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "791c2176",
   "metadata": {},
   "source": [
    "### Findings (NAI) ###\n",
    "- SD 7G vs SD 4G: EMA pruned. *Applies for all models*.\n",
    "- SD 7G vs NAI 7G: **Same \"text encoder\" (renamed layer) and \"VAE\".**\n",
    "- ACertainty: \"Seriously fine-tuned from SD with tons of (NAI) AIGC.\" **confirmed.** However VAE Decoder is different."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8644af64-fc44-4c77-9a2b-631f7c8ead0b",
   "metadata": {},
   "source": [
    "### Findings (SD Variant) ###\n",
    "- momoko-e: **Dreambooth**. Text encoder is **partially changed.**\n",
    "- Anything v3 / BasilMix / Anything v4 etc.: **Merged model. All layers are changed.**\n",
    "- NAI: Some `cumprod` layers dropped\n",
    "- ANY3: Same as NAI (merged)\n",
    "- AC: Same as NAI (???)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a441a75-512b-495c-8263-d448a3580108",
   "metadata": {},
   "source": [
    "### Findings (NMFSAN) ###\n",
    "- NMFSAN: No `cond_stage_model` = Load \"last text encoder\" or `None` (will generate glitched images). No `first_stage_model` = Must load VAE (`same_model_name.vae.pt`).\n",
    "- Currently called \"negative textual inversion\". Freeze TE train UNET > Make TI > Freeze TE train UNET again. i.e. No TE no VAE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71cba828",
   "metadata": {},
   "source": [
    "### Findings (SD 2.x) ###\n",
    "- WD v1.4: Text encoder and VAE encoder is changed.\n",
    "- CJD v2.1.1: VAE encoder is changed.\n",
    "- J's RD: Text encoder and VAE are **uncahnged**.\n",
    "- P1at's merge: VAE is unchanged, but everything else is changed.\n",
    "- SD 1.x vs 2.x: Text encoder is entirely swithced. Some layers' dimension is changed. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af6658c8",
   "metadata": {},
   "source": [
    "## Discussion ##\n",
    "- \"Ignoring prompts\" (where is `astolfo`? No human!) is caused by **bias in text encoder?**\n",
    "- \"Missing details\" (given some element of the entity is present, e.g. `astolfo` has `pink_hair` but `1girl`) is caused by **bias in UNET?**\n",
    "- That's why Anything v3 (20 / 20 momoco style with minimal negative prompts) is popular because the task (waifu AIGC) is a narrow objective which favours bias?\n",
    "- BPModel is commented \"hard to use\" becasue it relies on original SD text encoder and VAE? However diversity is maximum with most art style is succesfully trained? Original SD has such \"artist prompts\", e.g. Vincent Van Gogh.\n",
    "- Why ACertainty looks like NAI? AIGC dataset as informal Reinforcement Learning?\n",
    "- SD 2.x is so broken becasuse the CLIP? Or the UNET? Why WD 1.4 E1 ignores prompts (where's `astolfo`? No `1boy`!) but start listening prompts in E2 (must include `quality:0` but `pink_hair`, `1boy` is OK)?\n",
    "- J's RD fails just because the original SD 2.x text encoder is so bad? Applies for CJD also (where's `astolfo`? No `1boy`!)?\n",
    "- Why BasilMix works (nice merge with chosen hyperparameters)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35610b14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anifusion2-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:16:33) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "cdebe89fc2d04ff6b12883a2bbcb56d4b0017c9c7ce836c65a005977eb4818df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
