{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "019fd4e3-4c88-43a3-a2bb-3ec4c7d0fa92",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Mega similarity check (V1) #\n",
    "\n",
    "### Abstract ###\n",
    "\n",
    "- Distance metric is not meaningful, **but the identity does.**\n",
    "- However I still prefer L2 distance ~~because it looks like I'm doing a ML task~~.\n",
    "- Objective: Try to explain *subjective* experience with model difference, especially if any components have been **changed**, ignoring how much it has been changed.\n",
    "- Thanks [\"CC\"](https://github.com/crosstyan), [\"RC\"](https://github.com/CCRcmcpe) and [\"AO\"](https://github.com/AdjointOperator) for providing the initial script (and the idea).\n",
    "\n",
    "### Input ### \n",
    "- See next cell. Paths of models and abbreviation you like.\n",
    "\n",
    "### Output ###\n",
    "- TONS of JSON, showing `(layer_name, distance_between_2_models)`\n",
    "- TONS of IMG, showing `(pair_of_model, distance_for_each_type_of_diffusion_layer)`\n",
    "\n",
    "### Special case or comparasion ###\n",
    "- Text encoder for model `nai`: `\"cond_stage_model.transformer\", \"cond_stage_model.transformer.text_model\"`\n",
    "\n",
    "### Some layer name to interprept ###\n",
    "- `first_stage_model`: VAE\n",
    "- `cond_stage_model`: Text Encoder\n",
    "- `model.diffusion_model`: Diffusion model\n",
    "- `model_ema`: EMA model for training\n",
    "- `cumprod`, `betas`, `alphas`: `CosineAnnealingLR`\n",
    "\n",
    "### Some notation (Useful in the bin chart) ###\n",
    "- `attn1`: `sattn` = *Self attention*\n",
    "- `attn2`: `xattn` = *Cross attention*\n",
    "- `ff`: *Feed forward*\n",
    "- `norm`: [Normalisation layer](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html). `elementwise_affine=True` introduces trainable `bias` and `weight`. \n",
    "- `proj`: *Projection*\n",
    "- `emb_layers`: *Embedding layers*\n",
    "- `others`: `ff` + `norm` + `proj` + `emb_layers`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c8a246d",
   "metadata": {},
   "source": [
    "Path configuration. See `./model_map.json` for the model paths.\n",
    "\n",
    "TODO: Single folder, static path generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6714cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths here.\n",
    "ofp_folder = {\n",
    "    \"json\": \"./json_v1/\",\n",
    "    \"img\": \"./img_v1/\"\n",
    "}\n",
    "model_map_f = \"./model_map_v1.json\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdfa8987",
   "metadata": {},
   "source": [
    "Load libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b6517a1-1515-415c-819d-5b496e5676e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import torch\n",
    "from safetensors.torch import load_file #safe_open\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75c910ff",
   "metadata": {},
   "source": [
    "Some operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "300ccc5b-4363-4002-8018-30e72fd0fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Support 'cuda', but 'cpu' is arleady fast.\n",
    "g_device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "385760de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output folder\n",
    "for v in ofp_folder.values():\n",
    "    os.makedirs(os.path.dirname(v), exist_ok=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6973fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For \"micro_cmp\", go to the cell with cmp_json()\n",
    "cmp_mapping = []\n",
    "try:\n",
    "    with open(model_map_f, \"r\") as mmf:\n",
    "        read_content = mmf.read()\n",
    "        cmp_mapping = json.loads(read_content)\n",
    "except:\n",
    "    print(\"Error when loading model map. There won't be mass scale comparasion.\".format(model_map_f))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2610e9f",
   "metadata": {},
   "source": [
    "Functions inside the compare loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8ee09f7-dcac-4e7f-908e-5fb14bdd9cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path: Path, device: str, print_ptl_info=False) -> dict[str, torch.Tensor]:\n",
    "    if \".safetensors\" in path.suffixes:\n",
    "        return load_file(path, device=device)\n",
    "    else:\n",
    "        ckpt = torch.load(path, map_location=device)\n",
    "        if print_ptl_info and \"epoch\" in ckpt and \"global_step\" in ckpt:\n",
    "            print(f\"[I] {path.name}: epoch {ckpt['epoch']}, step {ckpt['global_step']}\")\n",
    "        return ckpt[\"state_dict\"] if \"state_dict\" in ckpt else ckpt\n",
    "\n",
    "# Reminder: Dodge different shape!\n",
    "def check_equal_shape(a: torch.Tensor, b: torch.Tensor, fn):\n",
    "    if a.shape != b.shape:\n",
    "        raise Exception(\"DIFFERENT SHAPE\")\n",
    "        #print(\"DIFFERENT SHAPE: return -1.0\")\n",
    "        #return -1.0\n",
    "    return fn(a.type(torch.float),b.type(torch.float))\n",
    "\n",
    "TENSOR_METRIC_MAP = {\n",
    "    #\"equal\": torch.equal,\n",
    "    \"l0\": lambda a, b: check_equal_shape(a, b, lambda a, b: torch.dist(a, b, p=0)),    \n",
    "    \"l1\": lambda a, b: check_equal_shape(a, b, lambda a, b: torch.dist(a, b, p=1)),\n",
    "    \"l2\": lambda a, b: check_equal_shape(a, b, lambda a, b: torch.dist(a, b, p=2)),\n",
    "    \"cossim\": lambda a, b: check_equal_shape(a, b, lambda a, b: torch.mean(torch.cosine_similarity(a, b, dim=0)))\n",
    "}\n",
    "\n",
    "FIG_METRIC_MAP = {\n",
    "    #\"equal\": lambda v: np.linalg.norm(v, 0), \n",
    "    \"l0\": lambda v: np.linalg.norm(v, 0),    \n",
    "    \"l1\": lambda v: np.linalg.norm(v, 1),\n",
    "    \"l2\": lambda v: np.linalg.norm(v, 2),\n",
    "    #I don't know how to make this meaningful...\n",
    "    \"cossim\": lambda v: np.linalg.norm(v, None)\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6da9388",
   "metadata": {},
   "source": [
    "Read a pair of models, extract the key paths, compare for difference, and return all the intermediate data (useful for next step).\n",
    "\n",
    "In graphical sense: `(da(kv)ab)err`. Obvious?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ef0b25b-61e4-4bfb-8bcc-cbbf8e148753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp_c(a_path, b_path, device, metric, no_ptl_info):\n",
    "    metric_fn = TENSOR_METRIC_MAP[metric]\n",
    "    \n",
    "    try:\n",
    "        a_path = a_path.decode('UTF-8')\n",
    "        b_path = b_path.decode('UTF-8')\n",
    "    except:\n",
    "        #No need\n",
    "        pass\n",
    "\n",
    "    a = load_model(Path(a_path), device, not no_ptl_info)\n",
    "    b = load_model(Path(b_path), device, not no_ptl_info)\n",
    "\n",
    "    ak = set(a.keys())\n",
    "    bk = set(b.keys())\n",
    "    \n",
    "    keys_inter = ak.intersection(bk)\n",
    "    da = list(ak.difference(bk))\n",
    "    db = list(bk.difference(ak))\n",
    "    kv = {}\n",
    "    err = []\n",
    "    for k in keys_inter:\n",
    "        try:\n",
    "            rt = metric_fn(a[k], b[k])\n",
    "            rt = rt.numpy().tolist()\n",
    "            kv[k] = rt\n",
    "        except:\n",
    "            #\"nan\" or True / False\n",
    "            print(\"DIFFERENT SHAPE at key {}. Ignored.\".format(k))\n",
    "            err.append(k)\n",
    "            pass        \n",
    "\n",
    "    #Special case: NAI renamed the TE (claimed using GPT-2)\n",
    "    if not ((\"animefull\" in str(a_path)) and (\"animefull\" in str(b_path))):\n",
    "        if \"animefull\" in str(a_path):\n",
    "            for dak in da:\n",
    "                if \"cond_stage_model.transformer\" in dak:\n",
    "                    kv[\"nai.\" + dak] = metric_fn(a[dak], b[dak.replace(\"cond_stage_model.transformer\", \"cond_stage_model.transformer.text_model\")]).numpy().tolist()\n",
    "        elif \"animefull\" in str(b_path):\n",
    "            for dbk in db:\n",
    "                if \"cond_stage_model.transformer\" in dbk:\n",
    "                    kv[\"nai.\" + dbk] = metric_fn(b[dbk], a[dbk.replace(\"cond_stage_model.transformer\", \"cond_stage_model.transformer.text_model\")]).numpy().tolist()\n",
    "\n",
    "    return kv, da, db, err, a, b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d7262b9",
   "metadata": {},
   "source": [
    "Plot graph from the results above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e76a270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp_attn(kv, a, b, ofi, d):\n",
    "    tmfn = TENSOR_METRIC_MAP[d]\n",
    "    fmfn = FIG_METRIC_MAP[d]\n",
    "    diffs = {}\n",
    "    dlabel = d.upper() #L2\n",
    "\n",
    "    no_unet = True\n",
    "\n",
    "    # Ensure there is UNET.\n",
    "    for k in kv.keys():\n",
    "        if k.startswith('model.diffusion_model'):\n",
    "            no_unet = False\n",
    "            break\n",
    "\n",
    "    if no_unet:\n",
    "        return\n",
    "\n",
    "    for k in kv.keys():\n",
    "        #TODO: Not only the UNET, do for any components.\n",
    "        if not k.startswith('model.diffusion_model'):\n",
    "            continue\n",
    "        delta = tmfn(a[k], b[k]).numpy().tolist()\n",
    "        if 'attn1' in k:\n",
    "            c = 'attn1' #'attn'\n",
    "        elif 'attn2' in k:\n",
    "            c = 'attn2' #'xattn'\n",
    "        else:\n",
    "            c = 'other'\n",
    "        diffs.setdefault(c, []).append(delta)\n",
    "\n",
    "    for k in diffs:\n",
    "        diffs[k] = np.concatenate([diffs[k]], axis=0)\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, 10), sharex=False)\n",
    "    fig.tight_layout(pad=5.0)\n",
    "    for i, (k, v) in enumerate(diffs.items()):\n",
    "        #bins=len(v) for finding outliers\n",
    "        #v: numpy.array. 80 layers for attn, 526 for others.\n",
    "        dval = fmfn(v)\n",
    "\n",
    "        axs[i].hist(v, bins=len(v), density=False)\n",
    "        axs[i].set(xlabel=dlabel, ylabel='a.u.')\n",
    "        axs[i].xaxis.labelpad = 20\n",
    "        axs[i].set_yscale('log')\n",
    "        axs[i].set_title(f'{k}: ${dlabel}={dval:.4f}$')\n",
    "    plt.savefig(ofi, bbox_inches='tight')\n",
    "    #WTF the plot retains? Why?\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bd474ec",
   "metadata": {},
   "source": [
    "Procedure of a comparasion. Original scripts has [custom garbage collection](https://docs.python.org/3/library/gc.html), but its default setting is fine for me. Also $O(N^2)$ comparasion is harsh.\n",
    "\n",
    "Variables explanation for \"nice guys\":\n",
    "\n",
    "|var|text|\n",
    "|---|---|\n",
    "|pa|Path of model A.|\n",
    "|pb|Path of model B.|\n",
    "|ofp|Folder path for output JSON reports.|\n",
    "|ofi|Folder path for output PNG plots.|\n",
    "|d|Distancing method, [p-norm](https://en.wikipedia.org/wiki/Norm_(mathematics)) or [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity).[pytorch](https://pytorch.org/docs/stable/generated/torch.dist.html), [numpy](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)|\n",
    "|npi|`no_ptl_info`. IDK what it means.|\n",
    "|kv|[Key-Value Pairs of intersection.](https://www.w3schools.com/js/js_json_objects.asp)|\n",
    "|da|Distinct content of model A.|\n",
    "|db|Distinct content of model B.|\n",
    "|err|Interset layers which throw errors. Usually they're in different shape.|\n",
    "|a|Instance of model A.|\n",
    "|b|Instance of model B.|\n",
    "|dj|Data for output JSON file.|\n",
    "|fj|File path for output JSON file.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cc121bc-e1c9-4e0a-84b9-ad2e50eb4cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp_json(pa, pb, ofp, ofi, d, npi):\n",
    "    kv, da, db, err, a, b = cmp_c(Path(pa), Path(pb), g_device, d, npi)\n",
    "    dj = {'kv':kv, 'da':da, 'db':db, 'err': err}\n",
    "    with open(ofp, \"w\") as fj:\n",
    "        json.dump(dj, fj, indent=4, sort_keys=True)\n",
    "    cmp_attn(kv, a, b, ofi, d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "150cc0fa",
   "metadata": {},
   "source": [
    "Test / Manual operation for a single comparasion.\n",
    "\n",
    "~~Also as example for the above variables.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55067342-8567-472c-9a07-dbbb4b210c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing: Obvious result\n",
    "cmp_json(\n",
    "    \"../../stable-diffusion-webui/tmp/SD1/aobp/ABPModel-ep59.safetensors\", \n",
    "    \"../../stable-diffusion-webui/models/Stable-diffusion/sample-nd-epoch59.safetensors\",\n",
    "    \"./json/test.json\",\n",
    "    \"./img/test.png\",\n",
    "    \"l2\",\n",
    "    True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72faedc9",
   "metadata": {},
   "source": [
    "The compare loop. `tqdm` may not work, at least for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a299e852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " model pairs: 100%|██████████| 6/6 [00:07<00:00,  1.26s/it]\n",
      " model pairs: 100%|██████████| 1/1 [00:16<00:00, 16.50s/it]]\n",
      " model pairs: 100%|██████████| 4/4 [00:45<00:00, 11.25s/it]]\n",
      " model pairs: 100%|██████████| 5/5 [00:44<00:00,  8.94s/it]]\n",
      " model pairs: 100%|██████████| 3/3 [00:29<00:00,  9.84s/it]]\n",
      " model pairs: 100%|██████████| 2/2 [00:15<00:00,  7.57s/it]]\n",
      " model pairs: 100%|██████████| 5/5 [00:39<00:00,  7.81s/it]]\n",
      " model pairs: 100%|██████████| 2/2 [00:19<00:00,  9.72s/it]]\n",
      " model pairs: 100%|██████████| 2/2 [00:16<00:00,  8.06s/it]]\n",
      " model group:  90%|█████████ | 9/10 [03:53<00:23, 23.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.7.1.proj_in.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.2.1.proj_in.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.3.1.proj_in.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.6.1.proj_out.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.middle_block.1.proj_out.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.4.1.proj_in.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.5.1.proj_in.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.9.1.proj_out.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.middle_block.1.proj_in.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.4.1.proj_in.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.6.1.proj_in.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.4.1.proj_out.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.9.1.proj_in.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.5.1.proj_out.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.11.1.proj_in.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.5.1.proj_out.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.2.1.proj_out.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.1.1.proj_in.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.7.1.proj_in.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.4.1.proj_out.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.8.1.proj_in.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.8.1.proj_out.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.3.1.proj_out.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.8.1.proj_in.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.8.1.proj_out.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.11.1.proj_out.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.5.1.proj_in.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.7.1.proj_out.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.10.1.proj_in.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.1.1.proj_out.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.input_blocks.7.1.proj_out.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.output_blocks.10.1.proj_out.weight. Ignored.\n",
      "DIFFERENT SHAPE at key model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight. Ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " model pairs: 100%|██████████| 7/7 [01:18<00:00, 11.19s/it]\n",
      " model group: 100%|██████████| 10/10 [05:11<00:00, 31.14s/it]\n"
     ]
    }
   ],
   "source": [
    "cmp_count = 0\n",
    "ts = time.time()\n",
    "for cm0 in tqdm(cmp_mapping, desc=\" model group\", position=0):\n",
    "    ofp0 = cm0[0]\n",
    "    for pab in tqdm(cm0[1], desc=\" model pairs\", position=1):\n",
    "        pak = pab[0][0]\n",
    "        pav = pab[0][1]\n",
    "        pbk = pab[1][0]\n",
    "        pbv = pab[1][1]\n",
    "        ofjp = \"{}{}_{}_{}.json\".format(ofp_folder['json'], ofp0, pak, pbk)\n",
    "        ofip = \"{}{}_{}_{}.png\".format(ofp_folder['img'], ofp0, pak, pbk)\n",
    "        #print(ofjp)\n",
    "        cmp_count = cmp_count + 1\n",
    "        cmp_json(pav, pbv, ofjp, ofip, \"l2\", True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39e6b8bb",
   "metadata": {},
   "source": [
    "End of the comparasion loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cf1db38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare: 37, time: 311 sec\n"
     ]
    }
   ],
   "source": [
    "te = time.time()\n",
    "print(\"Compare: {}, time: {} sec\".format(cmp_count, int(te - ts)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "503b9db4-4a84-4988-a51e-31493502beeb",
   "metadata": {},
   "source": [
    "### Findings (VAE) ###\n",
    "- `kl-f8` vs SD prune: Some layers are pruned.\n",
    "- `kl-f8` vs WD1: Both `encoder`, `decoder` is trained\n",
    "- WD1 vs WD2: Only `decoder` is trained\n",
    "- `kl-f8` vs NAI: Only `decoder` is trained. **However it is same as SD v1.4 bundled. See below.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "791c2176",
   "metadata": {},
   "source": [
    "### Findings (NAI) ###\n",
    "- SD 7G vs SD 4G: EMA pruned. *Applies for all models*.\n",
    "- SD 7G vs NAI 7G: **Same \"text encoder\" (renamed layer) and \"VAE\".**\n",
    "- ACertainty: \"Seriously fine-tuned from SD with tons of (NAI) AIGC.\" **confirmed.** However VAE Decoder is different."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8644af64-fc44-4c77-9a2b-631f7c8ead0b",
   "metadata": {},
   "source": [
    "### Findings (SD Variant) ###\n",
    "- momoko-e: **Dreambooth**. Text encoder is **partially changed.**\n",
    "- Anything v3 / BasilMix / Anything v4 etc.: **Merged model. All layers are changed.**\n",
    "- NAI: Some `cumprod` layers dropped\n",
    "- ANY3: Same as NAI (merged)\n",
    "- AC: Same as NAI (???)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a441a75-512b-495c-8263-d448a3580108",
   "metadata": {},
   "source": [
    "### Findings (NMFSAN) ###\n",
    "- NMFSAN: No `cond_stage_model` = Load \"last text encoder\" or `None` (will generate glitched images). No `first_stage_model` = Must load VAE (`same_model_name.vae.pt`).\n",
    "- Currently called \"negative textual inversion\". Freeze TE train UNET > Make TI > Freeze TE train UNET again. i.e. No TE no VAE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "193a86a4",
   "metadata": {},
   "source": [
    "### Findings (BPModel) ###\n",
    "- I have internal versions started from \"Stupid Diffusion\", another internal version of ACertainty.\n",
    "- This is not a strict and formal proof, but I expect the L2 distances will align with a **almost flat plane**, to show some meaning for such comparasion.\n",
    "- This **must not the exact same plane** because the BPModel was trained with changed dataset and configuration (for example, ARB setting / adding subset of datasets / \"negative-TI\" trick). However the iterlation should show a somewhat \"clear way of improvement\".\n",
    "\n",
    "|Model A|Model B|others|attn1|attn2|\n",
    "|---|---|---|---|---|\n",
    "|AC|mk0|96.0301|23.2504|26.0299|\n",
    "|mk0|mk3|51.6394|16.5754|13.6055|\n",
    "|mk3|mk5|20.2971|8.6033|4.6885|\n",
    "|mk5|nman|22.4227|7.5967|5.7737|\n",
    "|L1(A)|L1(B)|190.3893|56.0258|50.0976|\n",
    "|L2(A)|L2(B)|**113.1510**|**30.7742**|**30.2982**|\n",
    "|AC|nman|**113.7759**|**35.8125**|**30.5798**|\n",
    "\n",
    "- Somehow L2 can reflect \"direction between the models\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71cba828",
   "metadata": {},
   "source": [
    "### Findings (SD 2.x) ###\n",
    "- WD v1.4: Text encoder and VAE encoder is changed.\n",
    "- CJD v2.1.1: VAE encoder is changed.\n",
    "- J's RD: Text encoder and VAE are **uncahnged**.\n",
    "- P1at's merge: VAE is unchanged, but everything else is changed.\n",
    "- SD 1.x vs 2.x: Text encoder is entirely swithced. Some layers' dimension is changed. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af6658c8",
   "metadata": {},
   "source": [
    "## Discussion ##\n",
    "- \"Ignoring prompts\" (where is `astolfo`? No human!) is caused by **bias in text encoder?**\n",
    "- \"Missing details\" (given some element of the entity is present, e.g. `astolfo` has `pink_hair` but `1girl`) is caused by **bias in UNET?**\n",
    "- That's why Anything v3 (20 / 20 momoco style with minimal negative prompts) is popular because the task (waifu AIGC) is a narrow objective which favours bias?\n",
    "- BPModel is commented \"hard to use\" becasue it relies on original SD text encoder and VAE? However diversity is maximum with most art style is succesfully trained? Original SD has such \"artist prompts\", e.g. Vincent Van Gogh.\n",
    "- Why ACertainty looks like NAI? AIGC dataset as informal Reinforcement Learning?\n",
    "- SD 2.x is so broken becasuse the CLIP? Or the UNET? Why WD 1.4 E1 ignores prompts (where's `astolfo`? No `1boy`!) but start listening prompts in E2 (must include `quality:0` but `pink_hair`, `1boy` is OK)?\n",
    "- J's RD fails just because the original SD 2.x text encoder is so bad? Applies for CJD also (where's `astolfo`? No `1boy`!)?\n",
    "- Why BasilMix works (nice merge with chosen hyperparameters)?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4206b759",
   "metadata": {},
   "source": [
    "# Further work #\n",
    "- Compare for a set of models **with a clear relatiion**. For example, [merging ratio](https://huggingface.co/ThePioneer/CoolerWaifuDiffusion) and [training epoches](https://huggingface.co/AnnihilationOperator/ABPModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35610b14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anifusion2-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:16:33) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "cdebe89fc2d04ff6b12883a2bbcb56d4b0017c9c7ce836c65a005977eb4818df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
